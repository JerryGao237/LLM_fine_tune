# -*- coding: utf-8 -*-
"""
LLM Fine-tune Demo (æœ¬åœ°ç¦»çº¿å¯è¿è¡Œ)
- ğŸ§ª æ¨ç†å±•ç¤ºï¼šæœ¬åœ°åŠ è½½åŸºåº§æ¨¡å‹ï¼ˆmodel/ ç›®å½•æˆ–ç»å¯¹è·¯å¾„ï¼‰ï¼Œå¯å åŠ  runs/ ä¸‹çš„ LoRA é€‚é…å™¨æ¨ç†
- ğŸ› ï¸ ç°åœºå¾®è°ƒï¼šç²˜è´´å°æ ·ä¾‹ JSONï¼ˆSFT/DPO/ORPO/KTOï¼‰ï¼Œä¸€é”® LoRA/QLoRA å¾®è°ƒï¼Œè¾“å‡ºåˆ° runs/ ç›®å½•
- ğŸ“Š å¾®è°ƒå±•ç¤ºï¼šè¯»å– evaluation_results.json å±•ç¤ºæ’è¡Œæ¦œï¼Œå¯è°ƒç”¨ code/unified_evaluation.py é‡è·‘

è¿è¡Œï¼š
    pip install -r requirements.txt
    python main.py
"""
import os
import re
import json
import time
import glob
import subprocess
from typing import Dict, Any, List, Optional, Tuple

# ---- å¼ºåˆ¶æœ¬åœ°ç¦»çº¿ï¼Œä¸ä¼šè”ç½‘ä¸‹è½½ ----
os.environ.setdefault("HF_HUB_OFFLINE", "1")
os.environ.setdefault("TRANSFORMERS_OFFLINE", "1")

import gradio as gr
import pandas as pd
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ---------- å…¨å±€é»˜è®¤è·¯å¾„ï¼ˆå¯åœ¨ UI ä¸­è¦†ç›–ï¼‰ ----------
MODEL_DIR   = os.environ.get("DEMO_MODEL_DIR", "model")
RUNS_DIR    = os.environ.get("DEMO_RUNS_DIR", "runs")
EVAL_SCRIPT = os.environ.get("DEMO_EVAL_SCRIPT", "code/unified_evaluation.py")
EVAL_JSON   = os.environ.get("DEMO_EVAL_JSON", "evaluation_results.json")
DEFAULT_BASE_LOCAL = os.environ.get("DEMO_BASE_MODEL", "base")  # è¡¨ç¤ºä½¿ç”¨ model/base

# ------------------------- å·¥å…·å‡½æ•° -------------------------
def _fmt_exception(e: Exception) -> str:
    return f"{type(e).__name__}: {str(e)}"

def _resolve_local_model_path(user_value: str) -> str:
    """
    è§£ææœ¬åœ°æ¨¡å‹è·¯å¾„ï¼š
    - è‹¥ user_value æ˜¯ç°å­˜ç›®å½•ï¼Œç›´æ¥ä½¿ç”¨ï¼›
    - å¦åˆ™å°è¯• MODEL_DIR/user_valueï¼›
    - å¦åˆ™æŠ›é”™ï¼ˆä¸è”ç½‘ï¼‰ã€‚
    """
    v = (user_value or "").strip() or "base"
    if os.path.isdir(v):
        return v
    cand = os.path.join(MODEL_DIR, v)
    if os.path.isdir(cand):
        return cand
    raise FileNotFoundError(f"æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹ç›®å½•ï¼š'{v}' æˆ– '{cand}'ã€‚è¯·å°†æ¨¡å‹æ”¾åœ¨ {MODEL_DIR}/ å­ç›®å½•æˆ–æä¾›ç»å¯¹è·¯å¾„ã€‚")

def list_adapters(runs_dir: str = RUNS_DIR) -> List[Tuple[str, str]]:
    """
    è¿”å› (æ˜¾ç¤ºæ ‡ç­¾, è·¯å¾„)ï¼Œå¯»æ‰¾å« adapter_model.safetensors çš„ç›®å½•ï¼›
    å…ˆæŸ¥ run æ ¹ç›®å½•ï¼›å†ä»æœ€é«˜ checkpoint-* å¾€ä¸‹æ‰¾ã€‚
    """
    items: List[Tuple[str, str]] = []
    if not os.path.isdir(runs_dir):
        return items

    for run_name in sorted(os.listdir(runs_dir)):
        full = os.path.join(runs_dir, run_name)
        if not os.path.isdir(full):
            continue

        root_adapter = os.path.join(full, "adapter_model.safetensors")
        if os.path.exists(root_adapter):
            items.append((run_name, full))
            continue

        ckpts = sorted(
            (d for d in glob.glob(os.path.join(full, "checkpoint-*")) if os.path.isdir(d)),
            key=lambda p: int(re.findall(r"checkpoint-(\d+)", p)[0]) if re.findall(r"checkpoint-(\d+)", p) else -1,
            reverse=True
        )
        for ck in ckpts:
            ck_adapter = os.path.join(ck, "adapter_model.safetensors")
            if os.path.exists(ck_adapter):
                items.append((f"{run_name}/{os.path.basename(ck)}", ck))
                break
    return items

_MODEL_SLOT: Dict[str, Any] = {"key": None, "tok": None, "model": None}

def unload_model():
    global _MODEL_SLOT
    _MODEL_SLOT = {"key": None, "tok": None, "model": None}
    try:
        torch.cuda.empty_cache()
    except Exception:
        pass

def _maybe_quant_args(quant_mode: str) -> Dict[str, Any]:
    q = (quant_mode or "none").lower()
    if q == "8bit" or q == "4bit":
        # Windows ä¸‹æœªè£… bitsandbytes ä¼šå¤±è´¥ï¼›äº¤ç”± load_model æ•è·å¹¶è¿”å›æŠ¥é”™
        try:
            import bitsandbytes  # noqa: F401
        except Exception:
            raise RuntimeError("é€‰æ‹©äº† 8bit/4bitï¼Œä½†ç¯å¢ƒæœªå®‰è£… bitsandbytes æˆ–ç³»ç»Ÿä¸æ”¯æŒã€‚è¯·æ”¹å› 'none'ã€‚")
        if q == "8bit":
            return dict(load_in_8bit=True, device_map="auto")
        return dict(load_in_4bit=True, device_map="auto")
    # é»˜è®¤ float16ï¼ˆæœ‰ GPUï¼‰
    return dict(device_map="auto", torch_dtype=torch.float16 if torch.cuda.is_available() else None)

def load_model(base_model_hint: str, adapter_dir: Optional[str], quant_mode: str = "none"):
    """
    ä¸¥æ ¼æœ¬åœ°åŠ è½½ï¼šAutoTokenizer / AutoModelForCausalLMï¼ˆlocal_files_only=Trueï¼‰ï¼Œå¯å åŠ  LoRA é€‚é…å™¨ã€‚
    ä½¿ç”¨ä¸€ä¸ªå•æ§½ç¼“å­˜ï¼Œé¿å…é¢‘ç¹é‡å¤åŠ è½½ã€‚
    """
    global _MODEL_SLOT
    base_path = _resolve_local_model_path(base_model_hint)
    key = (base_path, adapter_dir or "", quant_mode)
    if _MODEL_SLOT["key"] == key and _MODEL_SLOT["model"] is not None:
        return _MODEL_SLOT["tok"], _MODEL_SLOT["model"]

    unload_model()

    quant_kwargs = _maybe_quant_args(quant_mode)
    tok = AutoTokenizer.from_pretrained(base_path, trust_remote_code=True, local_files_only=True)
    base = AutoModelForCausalLM.from_pretrained(
        base_path,
        trust_remote_code=True,
        local_files_only=True,
        low_cpu_mem_usage=True,
        **quant_kwargs
    )
    base.eval()
    model = PeftModel.from_pretrained(base, adapter_dir) if adapter_dir else base

    _MODEL_SLOT = {"key": key, "tok": tok, "model": model}
    return tok, model

def apply_chat_template(tok: AutoTokenizer, user_text: str) -> Dict[str, Any]:
    """
    å°†ç”¨æˆ·è¾“å…¥å°è£…ä¸ºèŠå¤©æ¨¡æ¿ï¼ˆè‹¥å¯ç”¨ï¼‰ï¼Œå¦åˆ™åŸæ ·ç¼–ç ã€‚
    """
    try:
        messages = [{"role": "user", "content": user_text}]
        text = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        return tok(text, return_tensors="pt")
    except Exception:
        return tok(user_text, return_tensors="pt")

# ------------------------- æ¨ç†å±•ç¤ºé€»è¾‘ -------------------------
def sync_runs_dropdown(runs_dir: str) -> Tuple[List[str], List[str]]:
    items = list_adapters(runs_dir)
    labels = ["(ä»…åŸºåº§æ¨¡å‹)"] + [lbl for (lbl, _) in items]
    paths = [""] + [pth for (_, pth) in items]
    return labels, paths

def infer_once(base_model_hint: str, adapter_choice_path: str, user_text: str,
               max_new_tokens: int, temperature: float, top_p: float,
               quant_mode: str, seed: int) -> Tuple[str, str]:
    if not (user_text or "").strip():
        return "", "è¯·è¾“å…¥å†…å®¹ã€‚"
    torch.manual_seed(int(seed))
    start = time.time()
    try:
        tok, model = load_model(base_model_hint, adapter_choice_path or None, quant_mode=quant_mode)
        # ---- è¿™é‡Œç»Ÿè®¡ LoRA å±‚ä¸æ´»åŠ¨é€‚é…å™¨ä¿¡æ¯ ----
        active = getattr(model, "active_adapter", None)
        try:
            import peft
            from peft.tuners.lora import LoraLayer
            lora_layers = sum(1 for _, m in model.named_modules() if isinstance(m, LoraLayer))
        except Exception:
            lora_layers = -1

        inputs = apply_chat_template(tok, user_text)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        gen_kwargs = {}
        if getattr(model.config, "pad_token_id", None) is None and getattr(tok, "pad_token_id", None) is None and getattr(tok, "eos_token_id", None) is not None:
            gen_kwargs["pad_token_id"] = tok.eos_token_id

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                do_sample=True,
                max_new_tokens=int(max_new_tokens),
                temperature=float(temperature),
                top_p=float(top_p),
                **gen_kwargs
            )
        # åªè§£ç æ–°ç”Ÿæˆéƒ¨åˆ†ï¼ˆé¿å…æŠŠæç¤ºè¯ä¹Ÿè§£ç å›å»ï¼Œçœ‹èµ·æ¥â€œæ›´åƒä¸€æ ·â€ï¼‰
        input_len = inputs["input_ids"].shape[1]
        text = tok.decode(outputs[0][input_len:], skip_special_tokens=True)

        info = f"{time.time() - start:.3f}s | adapter={adapter_choice_path or '(none)'} | active={active} | lora_layers={lora_layers}"
        return text, info
    except Exception as e:
        return "", f"æ¨ç†å¤±è´¥ï¼š{_fmt_exception(e)}"


# ------------------------- è¯„æµ‹å±•ç¤ºé€»è¾‘ -------------------------
def try_read_eval_json(path: str) -> Tuple[Optional[Dict[str, Any]], str]:
    if not os.path.exists(path):
        return None, f"æœªæ‰¾åˆ° {path}ã€‚å¯å…ˆç‚¹å‡»æŒ‰é’®è¿è¡Œè¯„æµ‹ï¼Œæˆ–åœ¨é¡¹ç›®æ ¹ç›®å½•æ”¾ç½®è¯¥æ–‡ä»¶ã€‚"
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data, f"å·²åŠ è½½ {path}ï¼ˆ{len(data)} ä¸ªæ¡ç›®ï¼‰ã€‚"
    except Exception as e:
        return None, f"è¯»å– {path} å¤±è´¥ï¼š{_fmt_exception(e)}"

def eval_to_dataframe(data: Dict[str, Any]) -> pd.DataFrame:
    rows = []
    for name, v in data.items():
        rows.append({
            "model_name": name,
            "model_path": v.get("model_path"),
            "preference_accuracy": v.get("preference_accuracy"),
            "loss_margin": v.get("loss_margin"),
            "perplexity": v.get("perplexity"),
            "test_loss": v.get("test_loss"),
            "training_time_hours": v.get("training_time_hours"),
            "total_steps": v.get("total_steps"),
            "best_eval_loss": v.get("best_eval_loss"),
            "final_eval_loss": v.get("final_eval_loss"),
        })
    df = pd.DataFrame(rows)
    sort_cols = [c for c in ["preference_accuracy", "loss_margin"] if c in df.columns]
    if sort_cols:
        df = df.sort_values(by=sort_cols, ascending=[False]*len(sort_cols), na_position="last")
    return df

def pick_generation_samples(data: Dict[str, Any], model_name: str) -> List[Dict[str, str]]:
    item = data.get(model_name) or {}
    return item.get("generation_samples", []) or []

def run_evaluation(methods: str, runs_dir: str, base_model_hint: str, skip_perplexity: bool, max_samples: int) -> str:
    if not os.path.exists(EVAL_SCRIPT):
        return f"æœªæ‰¾åˆ°è¯„æµ‹è„šæœ¬ï¼š{EVAL_SCRIPT}"
    try:
        base_path = _resolve_local_model_path(base_model_hint)
    except Exception as e:
        return _fmt_exception(e)
    cmd = ["python", EVAL_SCRIPT, "--runs_dir", runs_dir, "--base_model", base_path, "--max_samples", str(int(max_samples))]
    parts = [p.strip() for p in (methods or "").split(",") if p.strip()]
    if parts:
        cmd += ["--methods"] + parts
    if skip_perplexity:
        cmd += ["--skip_perplexity"]
    try:
        proc = subprocess.run(cmd, capture_output=True, text=True, check=False)
        out = []
        out.append("Command: " + " ".join(cmd))
        out.append("--- STDOUT ---\n" + (proc.stdout or ""))
        out.append("--- STDERR ---\n" + (proc.stderr or ""))
        return "\n".join(out)
    except Exception as e:
        return f"è¿è¡Œå¤±è´¥ï¼š{_fmt_exception(e)}"

# ------------------------- UI -------------------------
with gr.Blocks(title="LLM Fine-tune Demo (Local Only)") as demo:
    gr.Markdown("## ğŸ”’ æœ¬åœ°æ¨¡å‹ç¦»çº¿ Demo")
    gr.Markdown("æœ¬é¡µé¢**åªä»æœ¬åœ°åŠ è½½æ¨¡å‹**ï¼Œä¸ä¼šè”ç½‘ä¸‹è½½ã€‚åŸºåº§æ¨¡å‹ä» `model/<å­ç›®å½•>` æˆ–ç»å¯¹è·¯å¾„åŠ è½½ï¼›å¾®è°ƒæƒé‡ä» `runs/` ç›®å½•é€‰æ‹©ã€‚")

    # ========== æ¨ç†å±•ç¤º ==========
    with gr.Tab("ğŸ§ª æ¨ç†å±•ç¤º"):
        with gr.Row():
            base_model_hint = gr.Textbox(value=DEFAULT_BASE_LOCAL, label="åŸºåº§æ¨¡å‹ï¼ˆæœ¬åœ°è·¯å¾„ æˆ– model å­ç›®å½•åï¼‰", lines=1)
            quant_mode = gr.Radio(choices=["none", "8bit", "4bit"], value="none", label="é‡åŒ–åŠ è½½")
        with gr.Row():
            runs_dir = gr.Textbox(value=RUNS_DIR, label="æƒé‡ç›®å½•ï¼ˆrunsï¼‰", lines=1)
            refresh_btn = gr.Button("åˆ·æ–°æƒé‡åˆ—è¡¨")

        # éå¯è§çŠ¶æ€ä¸è¦æ”¾ Row/Column
        adapter_path = gr.State(value="")
        adapter_label = gr.Dropdown(choices=["(ä»…åŸºåº§æ¨¡å‹)"], value="(ä»…åŸºåº§æ¨¡å‹)", label="é€‰æ‹©å¾®è°ƒæƒé‡ï¼ˆruns å­ç›®å½•ï¼‰")

        with gr.Accordion("ç”Ÿæˆå‚æ•°", open=False):
            with gr.Row():
                max_new_tokens = gr.Slider(minimum=16, maximum=1024, step=1, value=256, label="max_new_tokens")
                temperature    = gr.Slider(minimum=0.0, maximum=2.0, step=0.05, value=0.7, label="temperature")
                top_p          = gr.Slider(minimum=0.1, maximum=1.0, step=0.05, value=0.9, label="top_p")
                seed           = gr.Number(value=42, precision=0, label="seed")
        user_text = gr.Textbox(label="è¾“å…¥ï¼ˆå•è½®ï¼‰", lines=6, placeholder="ä¾‹å¦‚ï¼šç»™æˆ‘3æ¡å­¦ä¹ ç¼–ç¨‹çš„å»ºè®®")
        go = gr.Button("ç”Ÿæˆ")
        with gr.Row():
            out_text = gr.Textbox(label="è¾“å‡º", lines=10)
            out_lat = gr.Textbox(label="è€—æ—¶")
        refresh_info = gr.Markdown("")

        def _refresh(runs_dir_str: str):
            labels, paths = sync_runs_dropdown(runs_dir_str)
            # é»˜è®¤é€‰æ‹©ç¬¬ä¸€é¡¹ï¼ˆä»…åŸºåº§ï¼‰
            return gr.update(choices=labels, value=labels[0]), "" , f"å·²æ‰¾åˆ° {max(0, len(labels)-1)} ä¸ªé€‚é…å™¨ã€‚"

        refresh_btn.click(_refresh, inputs=[runs_dir], outputs=[adapter_label, adapter_path, refresh_info])

        def _choose_adapter(label: str, runs_dir_str: str):
            labels, paths = sync_runs_dropdown(runs_dir_str)
            mapping = {lbl: pth for lbl, pth in zip(labels, paths)}
            return mapping.get(label, "")

        adapter_label.change(_choose_adapter, inputs=[adapter_label, runs_dir], outputs=[adapter_path])

        go.click(
            infer_once,
            inputs=[base_model_hint, adapter_path, user_text, max_new_tokens, temperature, top_p, quant_mode, seed],
            outputs=[out_text, out_lat]
        )

    # ========== ç°åœºå¾®è°ƒ ==========
    with gr.Tab("ğŸ› ï¸ ç°åœºå¾®è°ƒ"):
        gr.Markdown("åœ¨ä¸‹æ–¹ç²˜è´´**å°é‡æ ·æœ¬**ï¼ˆJSON åˆ—è¡¨ï¼‰ï¼Œé€‰æ‹©æ–¹æ³•åå¿«é€Ÿ LoRA/QLoRA å¾®è°ƒå¹¶ç”Ÿæˆæ–°é€‚é…å™¨ã€‚æ•°æ®æ ¼å¼æ”¯æŒï¼šSFT/DPO/ORPO/KTOã€‚")

        method2 = gr.Radio(choices=["sft","dpo","orpo","kto"], value="sft", label="æ–¹æ³•")
        with gr.Row():
            base_local = gr.Textbox(value=DEFAULT_BASE_LOCAL, label="åŸºåº§æ¨¡å‹ï¼ˆæœ¬åœ°è·¯å¾„ æˆ– model å­ç›®å½•åï¼‰")
            out_name = gr.Textbox(value="runs/quick_sft", label="è¾“å‡ºç›®å½•ï¼ˆå»ºè®®åœ¨ runs/ ä¸‹ï¼‰")
        with gr.Row():
            use_qlora = gr.Checkbox(value=True, label="QLoRA 4bit")
        data_json_txt = gr.Textbox(
            label="è®­ç»ƒæ•°æ® JSONï¼ˆlist[dict]ï¼‰",
            lines=14,
            value='[{"messages":[{"role":"user","content":"ç»™æˆ‘3æ¡å­¦ä¹ ç¼–ç¨‹çš„å»ºè®®"},{"role":"assistant","content":"1. åšæŒç»ƒä¹ \\n2. é˜…è¯»æºç \\n3. å¤šåšé¡¹ç›®"}]}]'
        )
        btn_fit = gr.Button("å¼€å§‹å¾®è°ƒ")
        fit_log = gr.Textbox(label="å¾®è°ƒæ—¥å¿—", lines=18, show_copy_button=True)
        btn_refresh_after = gr.Button("å¾®è°ƒå®Œæˆååˆ·æ–°æ¨ç†é¡µçš„æƒé‡åˆ—è¡¨")

        def _sample_fill(m):
            if m=="sft":
                return '[{"messages":[{"role":"user","content":"ç»™æˆ‘3æ¡å­¦ä¹ ç¼–ç¨‹çš„å»ºè®®"},{"role":"assistant","content":"1. åšæŒç»ƒä¹ \\n2. é˜…è¯»æºç \\n3. å¤šåšé¡¹ç›®"}]}]'
            if m in ("dpo","orpo"):
                return '[{"prompt":"å†™ä¸€æ®µè‡ªæˆ‘ä»‹ç»","chosen":"å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯â€¦ï¼ˆæ¸…æ™°æœ‰æ¡ç†ï¼‰","rejected":"å—¨â€¦ï¼ˆå«ç³Šå…¶è¾ï¼‰"}]'
            if m=="kto":
                return '[{"prompt":"ç»™æˆ‘ä¸€ä¸ªå­¦ä¹ è®¡åˆ’","completion":"å‘¨ä¸€åˆ°å‘¨äº”â€¦","label":1}]'
            return "[]"

        method2.change(_sample_fill, inputs=[method2], outputs=[data_json_txt])

        def _run_quick_fit(method, base_hint, out_dir, qlora_v, data_txt):
            import json, os, tempfile, subprocess
            try:
                base_path = _resolve_local_model_path(base_hint)
            except Exception as e:
                return f"è§£ææœ¬åœ°æ¨¡å‹å¤±è´¥ï¼š{e}"

            # --- è§£æå¹¶å†™å…¥ä¸´æ—¶ JSONï¼ˆç¡®ä¿ Windows ä¸é”æ–‡ä»¶ï¼‰ ---
            try:
                data = json.loads(data_txt)
                if not isinstance(data, list):
                    return "data_json å¿…é¡»æ˜¯ JSON åˆ—è¡¨ï¼ˆlist[dict]ï¼‰"
                fd, data_path = tempfile.mkstemp(suffix=".json")
                os.close(fd)  # å…³é”®ï¼šç«‹åˆ»å…³é—­å¥æŸ„ï¼Œé¿å… Windows æ–‡ä»¶é”
                with open(data_path, "w", encoding="utf-8") as f:
                    json.dump(data, f, ensure_ascii=False)
            except Exception as e:
                return f"è§£æ/å†™å…¥è®­ç»ƒæ•°æ®å¤±è´¥ï¼š{e}"

            # --- ç»„è£…å‘½ä»¤å¹¶è°ƒç”¨åç«¯ ---
            cmd = [
                "python", "code/quick_fit.py",
                "--method", method,
                "--model", base_path,
                "--dataset", data_path,   # ä¼ æœ¬åœ° JSON è·¯å¾„
                "--out", out_dir
            ]
            if qlora_v:
                cmd.append("--qlora")
            try:
                proc = subprocess.run(cmd, capture_output=True, text=True, check=False)
                out = []
                out.append("Command: " + " ".join(cmd))
                out.append("--- STDOUT ---\n" + (proc.stdout or ""))
                out.append("--- STDERR ---\n" + (proc.stderr or ""))
                return "\n".join(out)
            except Exception as e:
                return f"è¿è¡Œå¤±è´¥ï¼š{type(e).__name__}: {e}"
            finally:
                try:
                    os.remove(data_path)
                except Exception:
                    pass


        btn_fit.click(
            _run_quick_fit,
            inputs=[method2, base_local, out_name, use_qlora, data_json_txt],
            outputs=[fit_log]
        )

        # åˆ·æ–°æ¨ç†é¡µ Dropdown
        btn_refresh_after.click(_refresh, inputs=[runs_dir], outputs=[adapter_label, adapter_path, refresh_info])

    # ========== å¾®è°ƒå±•ç¤ºï¼ˆè¯„æµ‹ï¼‰ ==========
    with gr.Tab("ğŸ“Š å¾®è°ƒå±•ç¤º"):
        with gr.Row():
            methods = gr.Textbox(value="", label="åªè¯„æµ‹æŒ‡å®šæ–¹æ³•ï¼ˆé€—å·åˆ†éš”ï¼Œå¯ç•™ç©ºï¼‰")
            runs_dir2 = gr.Textbox(value=RUNS_DIR, label="runs ç›®å½•")
        with gr.Row():
            base_model2 = gr.Textbox(value=DEFAULT_BASE_LOCAL, label="åŸºåº§æ¨¡å‹ï¼ˆæœ¬åœ°è·¯å¾„ æˆ– model å­ç›®å½•åï¼‰")
            skip_pp = gr.Checkbox(value=True, label="è·³è¿‡ Perplexityï¼ˆåŠ å¿«è¯„æµ‹ï¼‰")
            max_samples = gr.Slider(minimum=20, maximum=1000, value=100, step=10, label="max_samples")
        run_eval_btn = gr.Button("è¿è¡Œè¯„æµ‹ï¼ˆè°ƒç”¨ unified_evaluation.py ï¼‰")
        cmd_log = gr.Textbox(label="è¯„æµ‹æ—¥å¿—ï¼ˆstdout/stderrï¼‰", lines=18, show_copy_button=True)
        reload_btn = gr.Button("é‡æ–°åŠ è½½ evaluation_results.json")
        status = gr.Markdown("")

        leaderboard = gr.Dataframe(label="Leaderboardï¼ˆæŒ‰ preference_accuracy / loss_margin æ’åºï¼‰", interactive=False)
        with gr.Row():
            model_pick = gr.Dropdown(choices=[], label="é€‰æ‹©æ¨¡å‹æŸ¥çœ‹ç”Ÿæˆæ ·ä¾‹")
            samples_box = gr.JSON(label="generation_samples")

        def _run_eval(methods_str, runs_dir_str, base_model_str, skip_pp_bool, max_samples_val):
            log = run_evaluation(methods_str, runs_dir_str, base_model_str, bool(skip_pp_bool), int(max_samples_val))
            return log

        run_eval_btn.click(_run_eval, inputs=[methods, runs_dir2, base_model2, skip_pp, max_samples], outputs=[cmd_log])

        def _reload_json():
            data, msg = try_read_eval_json(EVAL_JSON)
            if data is None:
                return gr.update(value=[]), gr.update(choices=[]), "[]", msg
            df = eval_to_dataframe(data)
            return df, gr.update(choices=list(data.keys()), value=(list(data.keys())[0] if data else None)), "[]", msg

        reload_btn.click(_reload_json, outputs=[leaderboard, model_pick, samples_box, status])

        def _pick_samples(model_name: str):
            data, _ = try_read_eval_json(EVAL_JSON)
            if not data or not model_name:
                return "[]"
            samples = pick_generation_samples(data, model_name)
            return json.dumps(samples, ensure_ascii=False, indent=2)

        model_pick.change(_pick_samples, inputs=[model_pick], outputs=[samples_box])

if __name__ == "__main__":
    import warnings
    warnings.filterwarnings("ignore", category=UserWarning)
    demo.launch(server_name="0.0.0.0", server_port=7860)
