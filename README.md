# å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒæ¡†æ¶

è¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒæ¡†æ¶ï¼Œæ”¯æŒå¤šç§å¾®è°ƒæ–¹æ³•ï¼ˆSFTã€DPOã€ORPOã€KTOã€GRPOï¼‰ï¼Œå¹¶æä¾›å®Œæ•´çš„è¯„æµ‹å·¥å…·ã€‚

## ğŸ“‹ ç›®å½•

- [é¡¹ç›®ç®€ä»‹](#é¡¹ç›®ç®€ä»‹)
- [æ”¯æŒçš„æ–¹æ³•](#æ”¯æŒçš„æ–¹æ³•)
- [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [è¯¦ç»†ä½¿ç”¨](#è¯¦ç»†ä½¿ç”¨)
- [è¯„æµ‹å·¥å…·](#è¯„æµ‹å·¥å…·)
- [è¾“å‡ºç»“æ„](#è¾“å‡ºç»“æ„)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

## é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ªç»Ÿä¸€çš„å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒæ¡†æ¶ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

- **å¤šæ–¹æ³•æ”¯æŒ**ï¼šæ”¯æŒ SFTã€DPOã€ORPOã€KTOã€GRPO äº”ç§å¾®è°ƒæ–¹æ³•
- **é«˜æ•ˆè®­ç»ƒ**ï¼šä½¿ç”¨ QLoRAï¼ˆ4-bit é‡åŒ– + LoRAï¼‰å®ç°å‚æ•°é«˜æ•ˆå¾®è°ƒ
- **è‡ªåŠ¨è¯„æµ‹**ï¼šæä¾›ç»Ÿä¸€è¯„æµ‹è„šæœ¬ï¼Œè‡ªåŠ¨è¯„ä¼°æ‰€æœ‰è®­ç»ƒç»“æœ
- **å®Œæ•´æŒ‡æ ‡**ï¼šåŒ…æ‹¬ Lossã€Perplexityã€åå¥½å‡†ç¡®ç‡ç­‰å¤šç»´åº¦è¯„ä¼°
- **çµæ´»é…ç½®**ï¼šæ”¯æŒæ•°æ®é‡‡æ ·ã€éªŒè¯è¯„ä¼°ã€è‡ªå®šä¹‰å‚æ•°ç­‰

## æ”¯æŒçš„æ–¹æ³•

| æ–¹æ³• | ç±»å‹ | æ•°æ®é›†è¦æ±‚ | å­¦ä¹ ç‡ | è¯´æ˜ |
|------|------|-----------|--------|------|
| SFT | ç›‘ç£å¾®è°ƒ | å•æ¡å›å¤ï¼ˆmessagesï¼‰ | 2e-4 | åŸºç¡€å¾®è°ƒæ–¹æ³• |
| DPO | åå¥½å¯¹é½ | åå¥½å¯¹ï¼ˆchosen/rejectedï¼‰ | 5e-5 | ç›´æ¥åå¥½ä¼˜åŒ– |
| ORPO | åå¥½å¯¹é½ | åå¥½å¯¹ï¼ˆchosen/rejectedï¼‰ | 8e-6 | å¥‡å¼‚æ¯”åå¥½ä¼˜åŒ– |
| KTO | åå¥½å¯¹é½ | å•æ¡æ ‡æ³¨ï¼ˆlabelï¼‰ | 5e-5 | Kahneman-Tversky ä¼˜åŒ– |
| GRPO | å¼ºåŒ–å­¦ä¹  | prompt åˆ—è¡¨ | 5e-5 | ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– |

## ç¯å¢ƒé…ç½®

### å¿…éœ€ä¾èµ–

```bash
pip install torch transformers datasets accelerate peft bitsandbytes trl
```

### æ¨èç¯å¢ƒ

- Python 3.8+
- CUDA 11.8+
- GPU æ˜¾å­˜ â‰¥ 16GBï¼ˆä½¿ç”¨ QLoRA å¯åœ¨æ›´å°æ˜¾å­˜ä¸Šè¿è¡Œï¼‰

## å¿«é€Ÿå¼€å§‹

### 1. SFT ç›‘ç£å¾®è°ƒ

```bash
python code/SFT.py \
  --method sft \
  --model_name Qwen/Qwen2-0.5B-Instruct \
  --dataset HuggingFaceH4/ultrafeedback_binarized \
  --split train_sft \
  --eval_split test_sft \
  --output_dir runs/sft_qwen_0.5b \
  --max_train_samples 1000 \
  --max_eval_samples 100
```

### 2. DPO åå¥½ä¼˜åŒ–

```bash
python code/SFT.py \
  --method dpo \
  --model_name Qwen/Qwen2-0.5B-Instruct \
  --dataset HuggingFaceH4/ultrafeedback_binarized \
  --split train_prefs \
  --eval_split test_prefs \
  --output_dir runs/dpo_qwen_0.5b \
  --max_train_samples 1000 \
  --max_eval_samples 100
```

### 3. è¿è¡Œè¯„æµ‹

```bash
# è¯„æµ‹æ‰€æœ‰æ¨¡å‹
python code/unified_evaluation.py

# åªè¯„æµ‹ç‰¹å®šæ–¹æ³•
python code/unified_evaluation.py --methods sft dpo

# è·³è¿‡è€—æ—¶çš„ Perplexity è®¡ç®—
python code/unified_evaluation.py --skip_perplexity
```

## è¯¦ç»†ä½¿ç”¨

### SFT.py å‘½ä»¤è¡Œå‚æ•°

#### åŸºç¡€å‚æ•°

- `--method`ï¼šå¾®è°ƒæ–¹æ³•ï¼Œå¯é€‰ `sft`ã€`dpo`ã€`orpo`ã€`kto`ã€`grpo`ï¼ˆå¿…éœ€ï¼‰
- `--model_name`ï¼šåŸºåº§æ¨¡å‹åç§°ï¼Œå¦‚ `Qwen/Qwen2-0.5B-Instruct`ï¼ˆå¿…éœ€ï¼‰
- `--dataset`ï¼šæ•°æ®é›†åç§°ï¼Œå¦‚ `HuggingFaceH4/ultrafeedback_binarized`ï¼ˆå¿…éœ€ï¼‰
- `--output_dir`ï¼šè¾“å‡ºç›®å½•ï¼Œå»ºè®®ä½¿ç”¨ `runs/{method}_{model}` æ ¼å¼ï¼ˆå¿…éœ€ï¼‰

#### æ•°æ®å‚æ•°

- `--split`ï¼šè®­ç»ƒæ•°æ®é›†åˆ†å‰²ï¼Œé»˜è®¤ `train`
  - SFT ä½¿ç”¨ï¼š`train_sft`
  - DPO/ORPO ä½¿ç”¨ï¼š`train_prefs`
  - GRPO ä½¿ç”¨ï¼š`train_gen`
- `--eval_split`ï¼šéªŒè¯æ•°æ®é›†åˆ†å‰²ï¼Œå¦‚ `test_sft`ã€`test_prefs`
- `--max_train_samples`ï¼šè®­ç»ƒæ ·æœ¬æ•°é‡ä¸Šé™ï¼ˆå¯é€‰ï¼Œç”¨äºå¿«é€Ÿå®éªŒï¼‰
- `--max_eval_samples`ï¼šéªŒè¯æ ·æœ¬æ•°é‡ä¸Šé™ï¼ˆå¯é€‰ï¼‰

#### è®­ç»ƒå‚æ•°

- `--num_train_epochs`ï¼šè®­ç»ƒè½®æ•°ï¼Œé»˜è®¤ `3`
- `--per_device_train_batch_size`ï¼šå•è®¾å¤‡è®­ç»ƒæ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤ `4`
- `--gradient_accumulation_steps`ï¼šæ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œé»˜è®¤ `4`
- `--learning_rate`ï¼šå­¦ä¹ ç‡ï¼ˆè‡ªåŠ¨æ ¹æ®æ–¹æ³•è®¾ç½®ï¼‰
- `--use_qlora`ï¼šå¯ç”¨ QLoRA 4-bit é‡åŒ–ï¼Œé»˜è®¤å¼€å¯

#### LoRA å‚æ•°

- `--lora_r`ï¼šLoRA ç§©ï¼Œé»˜è®¤ `16`
- `--lora_alpha`ï¼šLoRA alphaï¼Œé»˜è®¤ `32`
- `--lora_dropout`ï¼šLoRA dropoutï¼Œé»˜è®¤ `0.05`

### å®Œæ•´è®­ç»ƒç¤ºä¾‹

#### SFT ç›‘ç£å¾®è°ƒ

```bash
python code/SFT.py \
  --method sft \
  --model_name Qwen/Qwen2-0.5B-Instruct \
  --dataset HuggingFaceH4/ultrafeedback_binarized \
  --split train_sft \
  --eval_split test_sft \
  --output_dir runs/sft_qwen_0.5b \
  --num_train_epochs 3 \
  --per_device_train_batch_size 4 \
  --gradient_accumulation_steps 4 \
  --max_train_samples 1000 \
  --max_eval_samples 100
```

#### DPO ç›´æ¥åå¥½ä¼˜åŒ–

```bash
python code/SFT.py \
  --method dpo \
  --model_name Qwen/Qwen2-0.5B-Instruct \
  --dataset HuggingFaceH4/ultrafeedback_binarized \
  --split train_prefs \
  --eval_split test_prefs \
  --output_dir runs/dpo_qwen_0.5b \
  --num_train_epochs 3 \
  --max_train_samples 1000 \
  --max_eval_samples 100
```

#### ORPO å¥‡å¼‚æ¯”åå¥½ä¼˜åŒ–

```bash
python code/SFT.py \
  --method orpo \
  --model_name Qwen/Qwen2-0.5B-Instruct \
  --dataset HuggingFaceH4/ultrafeedback_binarized \
  --split train_prefs \
  --eval_split test_prefs \
  --output_dir runs/orpo_qwen_0.5b \
  --num_train_epochs 3 \
  --max_train_samples 1000 \
  --max_eval_samples 100
```

#### KTO Kahneman-Tversky ä¼˜åŒ–

```bash
python code/SFT.py \
  --method kto \
  --model_name Qwen/Qwen2-0.5B-Instruct \
  --dataset HuggingFaceH4/ultrafeedback_binarized \
  --split train_prefs \
  --eval_split test_prefs \
  --output_dir runs/kto_qwen_0.5b \
  --num_train_epochs 3 \
  --max_train_samples 1000 \
  --max_eval_samples 100
```

#### GRPO ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–

```bash
python code/SFT.py \
  --method grpo \
  --model_name Qwen/Qwen2-0.5B-Instruct \
  --dataset HuggingFaceH4/ultrafeedback_binarized \
  --split train_gen \
  --eval_split test_gen \
  --output_dir runs/grpo_qwen_0.5b \
  --num_train_epochs 3 \
  --max_train_samples 1000 \
  --max_eval_samples 100
```

## è¯„æµ‹å·¥å…·

### unified_evaluation.py

ç»Ÿä¸€è¯„æµ‹è„šæœ¬ï¼Œè‡ªåŠ¨å‘ç°å¹¶è¯„ä¼° `runs` ç›®å½•ä¸‹çš„æ‰€æœ‰è®­ç»ƒç»“æœã€‚

#### å‘½ä»¤è¡Œå‚æ•°

- `--runs_dir`ï¼šè®­ç»ƒç»“æœç›®å½•ï¼Œé»˜è®¤ `runs`
- `--methods`ï¼šåªè¯„æµ‹æŒ‡å®šæ–¹æ³•ï¼Œå¦‚ `--methods sft dpo`
- `--max_samples`ï¼šæ¯ä¸ªæŒ‡æ ‡ä½¿ç”¨çš„æœ€å¤§æ ·æœ¬æ•°ï¼Œé»˜è®¤ `100`
- `--max_generation_samples`ï¼šç”Ÿæˆæ ·ä¾‹çš„æ•°é‡ï¼Œé»˜è®¤ `3`
- `--skip_perplexity`ï¼šè·³è¿‡ Perplexity è®¡ç®—ï¼ˆåŠ å¿«è¯„æµ‹é€Ÿåº¦ï¼‰
- `--base_model`ï¼šåŸºåº§æ¨¡å‹åç§°ï¼Œé»˜è®¤ `Qwen/Qwen2-0.5B-Instruct`

#### ä½¿ç”¨ç¤ºä¾‹

```bash
# è¯„æµ‹æ‰€æœ‰æ¨¡å‹ï¼ˆå®Œæ•´è¯„ä¼°ï¼‰
python code/unified_evaluation.py

# åªè¯„æµ‹ SFT å’Œ DPO
python code/unified_evaluation.py --methods sft dpo

# å¿«é€Ÿè¯„ä¼°ï¼ˆè·³è¿‡ Perplexityï¼‰
python code/unified_evaluation.py --skip_perplexity

# ä½¿ç”¨æ›´å¤šæ ·æœ¬è¯„ä¼°
python code/unified_evaluation.py --max_samples 500

# æŒ‡å®šå…¶ä»–ç›®å½•
python code/unified_evaluation.py --runs_dir experiments/
```

#### è¯„æµ‹æŒ‡æ ‡

è¯„æµ‹è„šæœ¬ä¼šè‡ªåŠ¨è®¡ç®—ä»¥ä¸‹æŒ‡æ ‡ï¼š

**è®­ç»ƒæŒ‡æ ‡**ï¼ˆä» trainer_state.json æå–ï¼‰ï¼š
- `initial_train_loss`ï¼šåˆå§‹è®­ç»ƒæŸå¤±
- `final_train_loss`ï¼šæœ€ç»ˆè®­ç»ƒæŸå¤±
- `best_eval_loss`ï¼šæœ€ä½³éªŒè¯æŸå¤±
- `final_eval_loss`ï¼šæœ€ç»ˆéªŒè¯æŸå¤±
- `total_steps`ï¼šæ€»è®­ç»ƒæ­¥æ•°
- `training_time_seconds`ï¼šè®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰
- `training_time_hours`ï¼šè®­ç»ƒæ—¶é—´ï¼ˆå°æ—¶ï¼‰

**æµ‹è¯•æŒ‡æ ‡**ï¼ˆåœ¨æµ‹è¯•é›†ä¸Šè®¡ç®—ï¼‰ï¼š
- `perplexity`ï¼šå›°æƒ‘åº¦ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
- `test_loss`ï¼šæµ‹è¯•é›†æŸå¤±
- `preference_accuracy`ï¼šåå¥½å‡†ç¡®ç‡ï¼ˆchosen loss < rejected loss çš„æ¯”ä¾‹ï¼‰
- `preference_correct`ï¼šåå¥½æ­£ç¡®çš„æ ·æœ¬æ•°
- `preference_total`ï¼šåå¥½è¯„ä¼°çš„æ€»æ ·æœ¬æ•°
- `avg_chosen_loss`ï¼šchosen å›å¤çš„å¹³å‡æŸå¤±
- `avg_rejected_loss`ï¼šrejected å›å¤çš„å¹³å‡æŸå¤±
- `loss_margin`ï¼šæŸå¤±å·®å€¼ï¼ˆrejected - chosenï¼Œè¶Šå¤§è¶Šå¥½ï¼‰

**ç”Ÿæˆæ ·ä¾‹**ï¼š
- `generation_samples`ï¼šæ¨¡å‹ç”Ÿæˆçš„æ ·ä¾‹å›å¤ï¼ˆåŒ…å« prompt å’Œ responseï¼‰

#### JSON è¾“å‡ºæ ¼å¼

è¯„æµ‹ç»“æœä¿å­˜åœ¨ `evaluation_results.json`ï¼š

```json
{
  "sft_qwen_0.5b": {
    "model_name": "sft_qwen_0.5b",
    "model_path": "runs/sft_qwen_0.5b",
    "initial_train_loss": 2.1234,
    "final_train_loss": 1.2345,
    "best_eval_loss": 1.1234,
    "final_eval_loss": 1.1456,
    "total_steps": 750,
    "training_time_seconds": 3600,
    "training_time_hours": 1.0,
    "perplexity": 3.14,
    "test_loss": 1.1432,
    "preference_accuracy": 0.65,
    "preference_correct": 65,
    "preference_total": 100,
    "avg_chosen_loss": 1.05,
    "avg_rejected_loss": 1.35,
    "loss_margin": 0.30,
    "generation_samples": [
      {
        "prompt": "ç»™æˆ‘3æ¡å­¦ä¹ ç¼–ç¨‹çš„å»ºè®®",
        "response": "1. æ¯å¤©åšæŒç»ƒä¹ ..."
      }
    ]
  },
  "dpo_qwen_0.5b": {
    ...
  }
}
```

## è¾“å‡ºç»“æ„

è®­ç»ƒå®Œæˆåï¼Œæ¯ä¸ªæ¨¡å‹çš„è¾“å‡ºç›®å½•ç»“æ„å¦‚ä¸‹ï¼š

```
runs/{method}_{model}/
â”œâ”€â”€ adapter_config.json          # LoRA é…ç½®ï¼ˆ~1KBï¼‰
â”œâ”€â”€ adapter_model.safetensors    # LoRA æƒé‡ï¼ˆ~5-10MBï¼‰
â”œâ”€â”€ trainer_state.json           # è®­ç»ƒçŠ¶æ€å’ŒæŒ‡æ ‡ï¼ˆ~50KBï¼‰
â”œâ”€â”€ training_args.bin            # è®­ç»ƒå‚æ•°ï¼ˆ~10KBï¼‰
â”œâ”€â”€ tokenizer_config.json        # Tokenizer é…ç½®
â”œâ”€â”€ special_tokens_map.json      # ç‰¹æ®Š token æ˜ å°„
â””â”€â”€ checkpoint-{step}/           # æ£€æŸ¥ç‚¹ç›®å½•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    â”œâ”€â”€ adapter_model.safetensors
    â””â”€â”€ trainer_state.json
```

è¯¦ç»†è¯´æ˜è¯·å‚è€ƒ [RUNS_STRUCTURE.md](RUNS_STRUCTURE.md)ã€‚

## å¸¸è§é—®é¢˜

### Q1: æ•°æ®é›†æ ¼å¼è¦æ±‚

**A:** ä¸åŒæ–¹æ³•å¯¹æ•°æ®é›†æ ¼å¼æœ‰ä¸åŒè¦æ±‚ï¼š

- **SFT**ï¼šéœ€è¦ `messages` åˆ—ï¼Œæ ¼å¼ä¸ºå¯¹è¯åˆ—è¡¨
- **DPO/ORPO**ï¼šéœ€è¦ `prompt`ã€`chosen`ã€`rejected` åˆ—
- **KTO**ï¼šéœ€è¦ `prompt`ã€`completion`ã€`label` åˆ—
- **GRPO**ï¼šéœ€è¦ `prompt` åˆ—

æ¨èä½¿ç”¨ `HuggingFaceH4/ultrafeedback_binarized` æ•°æ®é›†ï¼Œå·²åŒ…å«æ‰€éœ€æ ¼å¼ã€‚

### Q2: å¦‚ä½•å‡å°‘æ˜¾å­˜å ç”¨

**A:** å¯ä»¥å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š

1. å‡å°æ‰¹æ¬¡å¤§å°ï¼š`--per_device_train_batch_size 2`
2. å¢åŠ æ¢¯åº¦ç´¯ç§¯ï¼š`--gradient_accumulation_steps 8`
3. å‡å° LoRA ç§©ï¼š`--lora_r 8`
4. ä½¿ç”¨æ•°æ®é‡‡æ ·ï¼š`--max_train_samples 500`

### Q3: è®­ç»ƒæ—¶é—´è¿‡é•¿

**A:** å¯ä»¥å°è¯•ï¼š

1. ä½¿ç”¨æ•°æ®é‡‡æ ·å¿«é€ŸéªŒè¯ï¼š`--max_train_samples 1000`
2. å‡å°‘è®­ç»ƒè½®æ•°ï¼š`--num_train_epochs 1`
3. è·³è¿‡éªŒè¯è¯„ä¼°ï¼šä¸æŒ‡å®š `--eval_split`
4. å¢å¤§æ‰¹æ¬¡å¤§å°ï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰

### Q4: å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ–¹æ³•

**A:** æ–¹æ³•é€‰æ‹©å»ºè®®ï¼š

- **SFT**ï¼šä½œä¸ºåŸºçº¿ï¼Œé€‚åˆæ‰€æœ‰åœºæ™¯
- **DPO**ï¼šæœ‰æˆå¯¹åå¥½æ•°æ®æ—¶çš„é¦–é€‰ï¼Œæ•ˆæœç¨³å®š
- **ORPO**ï¼šåœ¨ DPO åŸºç¡€ä¸Šæ”¹è¿›ï¼Œé€šå¸¸æ•ˆæœæ›´å¥½
- **KTO**ï¼šåªæœ‰å•æ¡è¯„åˆ†æ•°æ®æ—¶ä½¿ç”¨
- **GRPO**ï¼šå¼ºåŒ–å­¦ä¹ åœºæ™¯ï¼Œé€‚åˆéœ€è¦åœ¨çº¿åé¦ˆçš„ä»»åŠ¡

### Q5: åå¥½å‡†ç¡®ç‡çš„å«ä¹‰

**A:** åå¥½å‡†ç¡®ç‡è¡¡é‡æ¨¡å‹æ˜¯å¦ç»™ chosenï¼ˆé«˜è´¨é‡ï¼‰å›å¤åˆ†é…æ›´ä½çš„æŸå¤±ã€‚å…·ä½“è®¡ç®—ï¼š

- å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œåˆ†åˆ«è®¡ç®— chosen å’Œ rejected çš„æŸå¤±
- å¦‚æœ `chosen_loss < rejected_loss`ï¼Œåˆ™è®¤ä¸ºé¢„æµ‹æ­£ç¡®
- å‡†ç¡®ç‡ = æ­£ç¡®æ ·æœ¬æ•° / æ€»æ ·æœ¬æ•°

è¿™ä¸ªæŒ‡æ ‡å¯¹æ‰€æœ‰æ¨¡å‹éƒ½è®¡ç®—ï¼Œä½œä¸ºåŸºçº¿å¯¹æ¯”ã€‚é€šå¸¸ï¼š
- **SFT**ï¼š50-60%ï¼ˆæœªç»åå¥½å¯¹é½ï¼‰
- **DPO/ORPO**ï¼š65-75%ï¼ˆç»è¿‡åå¥½å¯¹é½ï¼‰

### Q6: å¦‚ä½•ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹

**A:** è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥è¿™æ ·åŠ è½½æ¨¡å‹ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# åŠ è½½åŸºåº§æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2-0.5B-Instruct",
    device_map="auto"
)

# åŠ è½½ LoRA æƒé‡
model = PeftModel.from_pretrained(base_model, "runs/sft_qwen_0.5b")

# åŠ è½½ tokenizer
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-0.5B-Instruct")

# ç”Ÿæˆå›å¤
messages = [{"role": "user", "content": "ä½ å¥½"}]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer(text, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

## å¼€å‘æŒ‡å—

### æ·»åŠ æ–°çš„å¾®è°ƒæ–¹æ³•

1. åœ¨ `SFT.py` çš„ `main()` å‡½æ•°ä¸­æ·»åŠ æ–°çš„ `elif` åˆ†æ”¯
2. ä» TRL å¯¼å…¥ç›¸åº”çš„ Trainer
3. é…ç½®è®­ç»ƒå‚æ•°å’Œæ•°æ®æ ¼å¼
4. æ›´æ–° README å’Œæ–‡æ¡£

### æ·»åŠ æ–°çš„è¯„æµ‹æŒ‡æ ‡

1. åœ¨ `unified_evaluation.py` çš„ `UnifiedEvaluator` ç±»ä¸­æ·»åŠ æ–°æ–¹æ³•
2. åœ¨ `evaluate_model()` ä¸­è°ƒç”¨æ–°æ–¹æ³•
3. æ›´æ–° JSON è¾“å‡ºæ ¼å¼
4. æ›´æ–° README çš„æŒ‡æ ‡è¯´æ˜

### è‡ªå®šä¹‰æ•°æ®é›†

å¦‚æœä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†ï¼Œéœ€è¦ç¡®ä¿ï¼š

1. æ•°æ®æ ¼å¼ç¬¦åˆæ‰€é€‰æ–¹æ³•çš„è¦æ±‚
2. ä½¿ç”¨ `code/check_dataset.py` æ£€æŸ¥æ•°æ®æ ¼å¼
3. å¿…è¦æ—¶ä¿®æ”¹ `SFT.py` ä¸­çš„æ•°æ®é¢„å¤„ç†é€»è¾‘

## å‚è€ƒèµ„æ–™

- [TRL æ–‡æ¡£](https://huggingface.co/docs/trl)
- [PEFT æ–‡æ¡£](https://huggingface.co/docs/peft)
- [Qwen2 æ¨¡å‹](https://huggingface.co/Qwen)
- [UltraFeedback æ•°æ®é›†](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized)

## è®¸å¯è¯

æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ã€‚
