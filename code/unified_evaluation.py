#!/usr/bin/env python3
"""
ç»Ÿä¸€è¯„æµ‹è„šæœ¬ - è‡ªåŠ¨è¯„ä¼° runs ç›®å½•ä¸‹çš„æ‰€æœ‰è®­ç»ƒç»“æœï¼Œè¾“å‡º JSON æ ¼å¼

åŠŸèƒ½ï¼š
  1. è‡ªåŠ¨å‘ç° runs ç›®å½•ä¸‹çš„æ‰€æœ‰è®­ç»ƒç»“æœ
  2. è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ï¼ˆLossã€Perplexityã€åå¥½å‡†ç¡®ç‡ï¼‰
  3. ç”Ÿæˆ JSON æ ¼å¼çš„å®Œæ•´è¯„ä¼°æŠ¥å‘Š

ç”¨æ³•:
    # è¯„ä¼° runs ç›®å½•ä¸‹çš„æ‰€æœ‰ç»“æœ
    python unified_evaluation.py

    # æŒ‡å®šå…¶ä»–ç›®å½•
    python unified_evaluation.py --runs_dir experiments/

    # åªè¯„ä¼°ç‰¹å®šæ–¹æ³•
    python unified_evaluation.py --methods sft dpo

    # è·³è¿‡è€—æ—¶çš„ Perplexity è®¡ç®—
    python unified_evaluation.py --skip_perplexity
"""

import argparse
import json
import os
from pathlib import Path
from typing import List, Dict, Optional
import numpy as np
import torch
from tqdm import tqdm
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel


class UnifiedEvaluator:
    def __init__(
        self,
        runs_dir: str = "runs",
        base_model: str = "Qwen/Qwen2-0.5B-Instruct",
        dataset_name: str = "HuggingFaceH4/ultrafeedback_binarized",
        max_samples: int = 100,
        skip_perplexity: bool = False,
        max_generation_samples: int = 3,
    ):
        self.runs_dir = Path(runs_dir)
        self.base_model = base_model
        self.dataset_name = dataset_name
        self.max_samples = max_samples
        self.skip_perplexity = skip_perplexity
        self.max_generation_samples = max_generation_samples

    def discover_models(self) -> List[Path]:
        """è‡ªåŠ¨å‘ç° runs ç›®å½•ä¸‹çš„æ‰€æœ‰è®­ç»ƒç»“æœ"""
        print(f"\n{'='*80}")
        print(f"ğŸ” æ‰«æç›®å½•: {self.runs_dir}")
        print(f"{'='*80}\n")

        if not self.runs_dir.exists():
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {self.runs_dir}")
            return []

        models = []
        for item in sorted(self.runs_dir.iterdir()):
            if item.is_dir():
                has_adapter = (item / "adapter_model.safetensors").exists()
                has_model = (item / "pytorch_model.bin").exists() or (item / "model.safetensors").exists()

                if has_adapter or has_model:
                    models.append(item)
                    model_type = "LoRA" if has_adapter else "Full"
                    print(f"  âœ“ {item.name} ({model_type})")

        print(f"\nå…±å‘ç° {len(models)} ä¸ªæ¨¡å‹\n")
        return models

    def load_training_state(self, model_path: Path) -> Optional[Dict]:
        """åŠ è½½è®­ç»ƒçŠ¶æ€"""
        state_file = model_path / "trainer_state.json"

        if not state_file.exists():
            checkpoints = sorted([d for d in model_path.iterdir()
                                if d.is_dir() and d.name.startswith("checkpoint-")])
            if checkpoints:
                state_file = checkpoints[-1] / "trainer_state.json"

        if state_file.exists():
            with open(state_file) as f:
                return json.load(f)
        return None

    def extract_training_metrics(self, state: Dict) -> Dict:
        """ä»è®­ç»ƒçŠ¶æ€ä¸­æå–æŒ‡æ ‡"""
        if not state:
            return {}

        log_history = state.get("log_history", [])

        # è®­ç»ƒ loss
        train_logs = [log for log in log_history if "loss" in log and "eval_loss" not in log]
        initial_train_loss = train_logs[0]["loss"] if train_logs else None
        final_train_loss = train_logs[-1]["loss"] if train_logs else None

        # éªŒè¯ loss
        eval_logs = [log for log in log_history if "eval_loss" in log]
        eval_losses = [log["eval_loss"] for log in eval_logs]
        best_eval_loss = min(eval_losses) if eval_losses else None
        final_eval_loss = eval_losses[-1] if eval_losses else None

        # è®­ç»ƒä¿¡æ¯
        total_steps = state.get("global_step", 0)
        best_checkpoint = state.get("best_model_checkpoint", None)

        # è®­ç»ƒæ—¶é—´
        training_time_seconds = 0
        if log_history and "train_runtime" in log_history[-1]:
            training_time_seconds = log_history[-1]["train_runtime"]

        return {
            "initial_train_loss": initial_train_loss,
            "final_train_loss": final_train_loss,
            "best_eval_loss": best_eval_loss,
            "final_eval_loss": final_eval_loss,
            "total_steps": total_steps,
            "best_checkpoint": best_checkpoint,
            "training_time_seconds": training_time_seconds,
            "training_time_hours": training_time_seconds / 3600 if training_time_seconds else 0,
        }

    def calculate_perplexity(self, model, tokenizer, dataset, max_samples: int = 100) -> tuple:
        """è®¡ç®—å›°æƒ‘åº¦"""
        total_loss = 0
        total_tokens = 0

        for i, sample in enumerate(tqdm(dataset, desc="  è®¡ç®— Perplexity", total=min(max_samples, len(dataset)))):
            if i >= max_samples:
                break

            if "messages" in sample:
                messages = sample["messages"]
            elif "chosen" in sample:
                messages = sample["chosen"]
            else:
                continue

            try:
                text = tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=False
                )
            except:
                text = "\n".join([f"{m['role']}: {m['content']}" for m in messages])

            inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
            inputs = {k: v.to(model.device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = model(**inputs, labels=inputs["input_ids"])
                loss = outputs.loss
                num_tokens = inputs["input_ids"].numel()
                total_loss += loss.item() * num_tokens
                total_tokens += num_tokens

        avg_loss = total_loss / total_tokens if total_tokens > 0 else 0
        perplexity = np.exp(avg_loss)

        return float(perplexity), float(avg_loss)

    def calculate_preference_accuracy(self, model, tokenizer, dataset, max_samples: int = 100) -> Dict:
        """è®¡ç®—åå¥½å‡†ç¡®ç‡"""
        correct = 0
        total = 0
        chosen_losses = []
        rejected_losses = []

        for sample in tqdm(dataset, desc="  è®¡ç®—åå¥½å‡†ç¡®ç‡", total=min(max_samples, len(dataset))):
            if total >= max_samples:
                break

            chosen = sample.get("chosen", [])
            rejected = sample.get("rejected", [])

            if not chosen or not rejected:
                continue

            # è®¡ç®— chosen çš„ loss
            try:
                chosen_text = tokenizer.apply_chat_template(
                    chosen, tokenize=False, add_generation_prompt=False
                )
            except:
                chosen_text = "\n".join([f"{m['role']}: {m['content']}" for m in chosen])

            chosen_inputs = tokenizer(chosen_text, return_tensors="pt", truncation=True, max_length=512)
            chosen_inputs = {k: v.to(model.device) for k, v in chosen_inputs.items()}

            with torch.no_grad():
                chosen_output = model(**chosen_inputs, labels=chosen_inputs["input_ids"])
                chosen_loss = chosen_output.loss.item()

            # è®¡ç®— rejected çš„ loss
            try:
                rejected_text = tokenizer.apply_chat_template(
                    rejected, tokenize=False, add_generation_prompt=False
                )
            except:
                rejected_text = "\n".join([f"{m['role']}: {m['content']}" for m in rejected])

            rejected_inputs = tokenizer(rejected_text, return_tensors="pt", truncation=True, max_length=512)
            rejected_inputs = {k: v.to(model.device) for k, v in rejected_inputs.items()}

            with torch.no_grad():
                rejected_output = model(**rejected_inputs, labels=rejected_inputs["input_ids"])
                rejected_loss = rejected_output.loss.item()

            if chosen_loss < rejected_loss:
                correct += 1

            total += 1
            chosen_losses.append(chosen_loss)
            rejected_losses.append(rejected_loss)

        if total == 0:
            return {}

        return {
            "preference_accuracy": float(correct / total),
            "preference_correct": correct,
            "preference_total": total,
            "avg_chosen_loss": float(np.mean(chosen_losses)),
            "avg_rejected_loss": float(np.mean(rejected_losses)),
            "loss_margin": float(np.mean(rejected_losses) - np.mean(chosen_losses)),
        }

    def generate_samples(self, model, tokenizer, prompts: List[str], max_new_tokens: int = 100) -> List[Dict]:
        """ç”Ÿæˆæ ·ä¾‹å›å¤"""
        results = []
        for prompt in prompts:
            messages = [{"role": "user", "content": prompt}]

            try:
                text = tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
            except:
                text = f"User: {prompt}\nAssistant:"

            inputs = tokenizer(text, return_tensors="pt").to(model.device)

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )

            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
            if "Assistant:" in generated:
                reply = generated.split("Assistant:")[-1].strip()
            else:
                reply = generated[len(text):].strip()

            results.append({"prompt": prompt, "response": reply})

        return results

    def evaluate_model(self, model_path: Path) -> Dict:
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        print(f"\n{'='*80}")
        print(f"ğŸ“Š è¯„ä¼°: {model_path.name}")
        print(f"{'='*80}\n")

        results = {
            "model_name": model_path.name,
            "model_path": str(model_path),
        }

        # 1. åŠ è½½è®­ç»ƒçŠ¶æ€
        print("ğŸ“ˆ åŠ è½½è®­ç»ƒæŒ‡æ ‡...")
        state = self.load_training_state(model_path)
        if state:
            train_metrics = self.extract_training_metrics(state)
            results.update(train_metrics)
            print(f"  âœ“ æœ€ä½³éªŒè¯ Loss: {train_metrics.get('best_eval_loss', 'N/A')}")
        else:
            print("  âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒçŠ¶æ€æ–‡ä»¶")

        # 2. åŠ è½½æ¨¡å‹
        if not self.skip_perplexity:
            try:
                print("\nğŸ¤– åŠ è½½æ¨¡å‹...")
                tokenizer = AutoTokenizer.from_pretrained(self.base_model)
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token

                adapter_config = model_path / "adapter_config.json"
                if adapter_config.exists():
                    base = AutoModelForCausalLM.from_pretrained(
                        self.base_model,
                        device_map="auto",
                        torch_dtype=torch.bfloat16
                    )
                    model = PeftModel.from_pretrained(base, str(model_path))
                    print("  âœ“ LoRA æ¨¡å‹")
                else:
                    model = AutoModelForCausalLM.from_pretrained(
                        str(model_path),
                        device_map="auto",
                        torch_dtype=torch.bfloat16
                    )
                    print("  âœ“ å®Œæ•´æ¨¡å‹")

                model.eval()

                # 3. åŠ è½½æµ‹è¯•æ•°æ®é›†
                print("\nğŸ“š åŠ è½½æµ‹è¯•é›†...")
                try:
                    test_ds = load_dataset(self.dataset_name, split="test_sft")
                    print("  âœ“ test_sft")
                except:
                    try:
                        test_ds = load_dataset(self.dataset_name, split="test_prefs")
                        print("  âœ“ test_prefs")
                    except:
                        print("  âš ï¸  æ— æ³•åŠ è½½æµ‹è¯•é›†")
                        test_ds = None

                # 4. è®¡ç®— Perplexity
                if test_ds:
                    print(f"\nğŸ§® è®¡ç®—å›°æƒ‘åº¦ ({self.max_samples} æ ·æœ¬)...")
                    perplexity, avg_loss = self.calculate_perplexity(
                        model, tokenizer, test_ds, self.max_samples
                    )
                    results["perplexity"] = perplexity
                    results["test_loss"] = avg_loss
                    print(f"  âœ“ Perplexity: {perplexity:.2f}")

                # 5. è®¡ç®—åå¥½å‡†ç¡®ç‡ï¼ˆæ‰€æœ‰æ¨¡å‹éƒ½è®¡ç®—ï¼Œä½œä¸ºå¯¹æ¯”åŸºçº¿ï¼‰
                try:
                    pref_ds = load_dataset(self.dataset_name, split="test_prefs")
                    print(f"\nğŸ¯ è®¡ç®—åå¥½å‡†ç¡®ç‡ ({self.max_samples} æ ·æœ¬)...")
                    pref_metrics = self.calculate_preference_accuracy(
                        model, tokenizer, pref_ds, self.max_samples
                    )
                    results.update(pref_metrics)
                    if pref_metrics:
                        print(f"  âœ“ å‡†ç¡®ç‡: {pref_metrics['preference_accuracy']:.2%}")
                except Exception as e:
                    print(f"  âš ï¸  è·³è¿‡åå¥½å‡†ç¡®ç‡: {e}")

                # 6. ç”Ÿæˆæ ·ä¾‹
                print(f"\nğŸ’¬ ç”Ÿæˆæ ·ä¾‹ ({self.max_generation_samples} ä¸ª)...")
                test_prompts = [
                    "ç»™æˆ‘3æ¡å­¦ä¹ ç¼–ç¨‹çš„å»ºè®®",
                    "å¦‚ä½•ä¿æŒå¥åº·çš„ç”Ÿæ´»æ–¹å¼ï¼Ÿ",
                    "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
                ][:self.max_generation_samples]

                samples = self.generate_samples(model, tokenizer, test_prompts, max_new_tokens=100)
                results["generation_samples"] = samples
                print(f"  âœ“ å®Œæˆ")

                # æ¸…ç†æ˜¾å­˜
                del model
                if 'base' in locals():
                    del base
                torch.cuda.empty_cache()

            except Exception as e:
                print(f"  âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")

        return results

    def run(self, filter_methods: Optional[List[str]] = None) -> Dict:
        """è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹"""
        # 1. å‘ç°æ¨¡å‹
        models = self.discover_models()

        if not models:
            print("âŒ æœªå‘ç°ä»»ä½•è®­ç»ƒç»“æœ")
            return {}

        # 2. è¿‡æ»¤æ¨¡å‹
        if filter_methods:
            models = [m for m in models if any(method in m.name.lower() for method in filter_methods)]
            print(f"è¿‡æ»¤å: {len(models)} ä¸ªæ¨¡å‹\n")

        # 3. è¯„ä¼°æ¯ä¸ªæ¨¡å‹
        all_results = {}
        for model_path in models:
            try:
                results = self.evaluate_model(model_path)
                all_results[model_path.name] = results
            except Exception as e:
                print(f"âŒ è¯„ä¼° {model_path.name} å¤±è´¥: {e}\n")

        # 4. ä¿å­˜ç»“æœ
        print(f"\n{'='*80}")
        print("ğŸ’¾ ä¿å­˜ç»“æœ")
        print(f"{'='*80}\n")

        output_file = "evaluation_results.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)

        print(f"âœ“ ç»“æœå·²ä¿å­˜: {output_file}")

        # 5. æ‰“å°æ‘˜è¦
        print(f"\n{'='*80}")
        print("ğŸ“Š è¯„ä¼°æ‘˜è¦")
        print(f"{'='*80}\n")

        print(f"{'æ¨¡å‹':<30} {'éªŒè¯Loss':<12} {'å›°æƒ‘åº¦':<12} {'åå¥½å‡†ç¡®ç‡':<12}")
        print("-" * 66)

        for name, results in sorted(all_results.items()):
            eval_loss = results.get("best_eval_loss")
            ppl = results.get("perplexity")
            pref_acc = results.get("preference_accuracy")

            eval_loss_str = f"{eval_loss:.4f}" if eval_loss else "N/A"
            ppl_str = f"{ppl:.2f}" if ppl else "N/A"
            pref_acc_str = f"{pref_acc*100:.2f}%" if pref_acc else "N/A"

            print(f"{name:<30} {eval_loss_str:<12} {ppl_str:<12} {pref_acc_str:<12}")

        print(f"\nâœ… è¯„ä¼°å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_file}\n")

        return all_results


def main():
    parser = argparse.ArgumentParser(description="ç»Ÿä¸€è¯„æµ‹è„šæœ¬ - è¾“å‡º JSON æ ¼å¼ç»“æœ")
    parser.add_argument("--runs_dir", type=str, default="runs", help="è®­ç»ƒç»“æœç›®å½•")
    parser.add_argument("--methods", nargs="+", help="åªè¯„ä¼°æŒ‡å®šæ–¹æ³• (å¦‚: sft dpo)")
    parser.add_argument("--max_samples", type=int, default=100, help="æ¯ä¸ªæŒ‡æ ‡ä½¿ç”¨çš„æœ€å¤§æ ·æœ¬æ•°")
    parser.add_argument("--max_generation_samples", type=int, default=3, help="ç”Ÿæˆæ ·ä¾‹çš„æ•°é‡")
    parser.add_argument("--skip_perplexity", action="store_true", help="è·³è¿‡ Perplexity è®¡ç®—")
    parser.add_argument("--base_model", type=str, default="Qwen/Qwen2-0.5B-Instruct", help="åŸºåº§æ¨¡å‹")

    args = parser.parse_args()

    evaluator = UnifiedEvaluator(
        runs_dir=args.runs_dir,
        base_model=args.base_model,
        max_samples=args.max_samples,
        skip_perplexity=args.skip_perplexity,
        max_generation_samples=args.max_generation_samples,
    )

    evaluator.run(filter_methods=args.methods)


if __name__ == "__main__":
    main()
