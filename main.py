# -*- coding: utf-8 -*-
"""
LLM Fine-tune Demo (æœ¬åœ°ç¦»çº¿å¯è¿è¡Œ)
- ğŸ§ª æ¨ç†å±•ç¤ºï¼šæœ¬åœ°åŠ è½½åŸºåº§æ¨¡å‹ï¼ˆmodel/ ç›®å½•æˆ–ç»å¯¹è·¯å¾„ï¼‰ï¼Œå¯å åŠ  runs/ ä¸‹çš„ LoRA é€‚é…å™¨æ¨ç†
- ğŸ› ï¸ ç°åœºå¾®è°ƒï¼šç²˜è´´å°æ ·ä¾‹ JSONï¼ˆSFT/DPO/ORPO/KTOï¼‰ï¼Œä¸€é”® LoRA/QLoRA å¾®è°ƒï¼Œè¾“å‡ºåˆ° runs/ ç›®å½•
- ğŸ“Š å¾®è°ƒå±•ç¤ºï¼šè¯»å– evaluation_results.json å±•ç¤ºæ’è¡Œæ¦œï¼Œå¯è°ƒç”¨ code/unified_evaluation.py é‡è·‘

è¿è¡Œï¼š
    pip install -r requirements.txt
    python main.py
"""
import os
import re
import json
import time
import glob
import subprocess
from typing import Dict, Any, List, Optional, Tuple

# ---- å¼ºåˆ¶æœ¬åœ°ç¦»çº¿ï¼Œä¸ä¼šè”ç½‘ä¸‹è½½ ----
os.environ.setdefault("HF_HUB_OFFLINE", "1")
os.environ.setdefault("TRANSFORMERS_OFFLINE", "1")

import gradio as gr
import pandas as pd
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ---------- å…¨å±€é»˜è®¤è·¯å¾„ï¼ˆå¯åœ¨ UI ä¸­è¦†ç›–ï¼‰ ----------
MODEL_DIR   = os.environ.get("DEMO_MODEL_DIR", "model")
RUNS_DIR    = os.environ.get("DEMO_RUNS_DIR", "runs")
EVAL_SCRIPT = os.environ.get("DEMO_EVAL_SCRIPT", "code/unified_evaluation.py")
EVAL_JSON   = os.environ.get("DEMO_EVAL_JSON", "results/evaluation_results.json")
DEFAULT_BASE_LOCAL = os.environ.get("DEMO_BASE_MODEL", "base")  # è¡¨ç¤ºä½¿ç”¨ model/base

# ------------------------- å·¥å…·å‡½æ•° -------------------------
def _fmt_exception(e: Exception) -> str:
    return f"{type(e).__name__}: {str(e)}"

def _resolve_local_model_path(user_value: str) -> str:
    """
    è§£ææœ¬åœ°æ¨¡å‹è·¯å¾„ï¼š
    - è‹¥ user_value æ˜¯ç°å­˜ç›®å½•ï¼Œç›´æ¥ä½¿ç”¨ï¼›
    - å¦åˆ™å°è¯• MODEL_DIR/user_valueï¼›
    - å¦åˆ™æŠ›é”™ï¼ˆä¸è”ç½‘ï¼‰ã€‚
    """
    v = (user_value or "").strip() or "base"
    if os.path.isdir(v):
        return v
    cand = os.path.join(MODEL_DIR, v)
    if os.path.isdir(cand):
        return cand
    raise FileNotFoundError(f"æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹ç›®å½•ï¼š'{v}' æˆ– '{cand}'ã€‚è¯·å°†æ¨¡å‹æ”¾åœ¨ {MODEL_DIR}/ å­ç›®å½•æˆ–æä¾›ç»å¯¹è·¯å¾„ã€‚")

def list_adapters(runs_dir: str = RUNS_DIR) -> List[Tuple[str, str]]:
    """
    è¿”å› (æ˜¾ç¤ºæ ‡ç­¾, è·¯å¾„)ï¼Œå¯»æ‰¾å« adapter_model.safetensors çš„ç›®å½•ï¼›
    å…ˆæŸ¥ run æ ¹ç›®å½•ï¼›å†ä»æœ€é«˜ checkpoint-* å¾€ä¸‹æ‰¾ã€‚
    """
    items: List[Tuple[str, str]] = []
    if not os.path.isdir(runs_dir):
        return items

    for run_name in sorted(os.listdir(runs_dir)):
        full = os.path.join(runs_dir, run_name)
        if not os.path.isdir(full):
            continue

        root_adapter = os.path.join(full, "adapter_model.safetensors")
        if os.path.exists(root_adapter):
            items.append((run_name, full))
            continue

        ckpts = sorted(
            (d for d in glob.glob(os.path.join(full, "checkpoint-*")) if os.path.isdir(d)),
            key=lambda p: int(re.findall(r"checkpoint-(\d+)", p)[0]) if re.findall(r"checkpoint-(\d+)", p) else -1,
            reverse=True
        )
        for ck in ckpts:
            ck_adapter = os.path.join(ck, "adapter_model.safetensors")
            if os.path.exists(ck_adapter):
                items.append((f"{run_name}/{os.path.basename(ck)}", ck))
                break
    return items

_MODEL_SLOT: Dict[str, Any] = {"key": None, "tok": None, "model": None}

def unload_model():
    global _MODEL_SLOT
    _MODEL_SLOT = {"key": None, "tok": None, "model": None}
    try:
        torch.cuda.empty_cache()
    except Exception:
        pass

def _maybe_quant_args(quant_mode: str) -> Dict[str, Any]:
    q = (quant_mode or "none").lower()
    if q == "8bit" or q == "4bit":
        # Windows ä¸‹æœªè£… bitsandbytes ä¼šå¤±è´¥ï¼›äº¤ç”± load_model æ•è·å¹¶è¿”å›æŠ¥é”™
        try:
            import bitsandbytes  # noqa: F401
        except Exception:
            raise RuntimeError("é€‰æ‹©äº† 8bit/4bitï¼Œä½†ç¯å¢ƒæœªå®‰è£… bitsandbytes æˆ–ç³»ç»Ÿä¸æ”¯æŒã€‚è¯·æ”¹å› 'none'ã€‚")
        if q == "8bit":
            return dict(load_in_8bit=True, device_map="auto")
        return dict(load_in_4bit=True, device_map="auto")
    # é»˜è®¤ float16ï¼ˆæœ‰ GPUï¼‰
    return dict(device_map="auto", torch_dtype=torch.float16 if torch.cuda.is_available() else None)

def load_model(base_model_hint: str, adapter_dir: Optional[str], quant_mode: str = "none"):
    """
    ä¸¥æ ¼æœ¬åœ°åŠ è½½ï¼šAutoTokenizer / AutoModelForCausalLMï¼ˆlocal_files_only=Trueï¼‰ï¼Œå¯å åŠ  LoRA é€‚é…å™¨ã€‚
    ä½¿ç”¨ä¸€ä¸ªå•æ§½ç¼“å­˜ï¼Œé¿å…é¢‘ç¹é‡å¤åŠ è½½ã€‚
    """
    global _MODEL_SLOT
    base_path = _resolve_local_model_path(base_model_hint)
    key = (base_path, adapter_dir or "", quant_mode)
    if _MODEL_SLOT["key"] == key and _MODEL_SLOT["model"] is not None:
        return _MODEL_SLOT["tok"], _MODEL_SLOT["model"]

    unload_model()
    print('base_path:', base_path)
    quant_kwargs = _maybe_quant_args(quant_mode)
    tok = AutoTokenizer.from_pretrained(base_path, trust_remote_code=True, local_files_only=True)
    base = AutoModelForCausalLM.from_pretrained(
        base_path,
        trust_remote_code=True,
        local_files_only=True,
        low_cpu_mem_usage=True,
        **quant_kwargs
    )

    base.eval()
    model = PeftModel.from_pretrained(base, adapter_dir) if adapter_dir else base

    _MODEL_SLOT = {"key": key, "tok": tok, "model": model}
    return tok, model

def apply_chat_template(tok: AutoTokenizer, user_text: str) -> Dict[str, Any]:
    """
    å°†ç”¨æˆ·è¾“å…¥å°è£…ä¸ºèŠå¤©æ¨¡æ¿ï¼ˆè‹¥å¯ç”¨ï¼‰ï¼Œå¦åˆ™åŸæ ·ç¼–ç ã€‚
    """
    try:
        messages = [{"role": "user", "content": user_text}]
        text = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        return tok(text, return_tensors="pt")
    except Exception:
        return tok(user_text, return_tensors="pt")

# ------------------------- æ¨ç†å±•ç¤ºé€»è¾‘ -------------------------
def sync_runs_dropdown(runs_dir: str) -> Tuple[List[str], List[str]]:
    items = list_adapters(runs_dir)
    labels = ["(ä»…åŸºåº§æ¨¡å‹)"] + [lbl for (lbl, _) in items]
    paths = [""] + [pth for (_, pth) in items]
    return labels, paths

def infer_once(base_model_hint: str, adapter_choice_path: str, user_text: str,
               max_new_tokens: int, temperature: float, top_p: float,
               quant_mode: str, seed: int) -> Tuple[str, str]:
    if not (user_text or "").strip():
        return "", "è¯·è¾“å…¥å†…å®¹ã€‚"
    torch.manual_seed(int(seed))
    start = time.time()
    try:
        tok, model = load_model(base_model_hint, adapter_choice_path or None, quant_mode=quant_mode)
        # ---- è¿™é‡Œç»Ÿè®¡ LoRA å±‚ä¸æ´»åŠ¨é€‚é…å™¨ä¿¡æ¯ ----
        active = getattr(model, "active_adapter", None)
        try:
            import peft
            from peft.tuners.lora import LoraLayer
            lora_layers = sum(1 for _, m in model.named_modules() if isinstance(m, LoraLayer))
        except Exception:
            lora_layers = -1

        inputs = apply_chat_template(tok, user_text)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        gen_kwargs = {}
        if getattr(model.config, "pad_token_id", None) is None and getattr(tok, "pad_token_id", None) is None and getattr(tok, "eos_token_id", None) is not None:
            gen_kwargs["pad_token_id"] = tok.eos_token_id

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                do_sample=True,
                max_new_tokens=int(max_new_tokens),
                temperature=float(temperature),
                top_p=float(top_p),
                **gen_kwargs
            )
        # åªè§£ç æ–°ç”Ÿæˆéƒ¨åˆ†ï¼ˆé¿å…æŠŠæç¤ºè¯ä¹Ÿè§£ç å›å»ï¼Œçœ‹èµ·æ¥â€œæ›´åƒä¸€æ ·â€ï¼‰
        input_len = inputs["input_ids"].shape[1]
        text = tok.decode(outputs[0][input_len:], skip_special_tokens=True)

        info = f"{time.time() - start:.3f}s | adapter={adapter_choice_path or '(none)'} | active={active} | lora_layers={lora_layers}"
        return text, info
    except Exception as e:
        return "", f"æ¨ç†å¤±è´¥ï¼š{_fmt_exception(e)}"


# ------------------------- è¯„æµ‹å±•ç¤ºé€»è¾‘ -------------------------
def try_read_eval_json(path: str) -> Tuple[Optional[Dict[str, Any]], str]:
    if not os.path.exists(path):
        return None, f"æœªæ‰¾åˆ° {path}ã€‚å¯å…ˆç‚¹å‡»æŒ‰é’®è¿è¡Œè¯„æµ‹ï¼Œæˆ–åœ¨é¡¹ç›®æ ¹ç›®å½•æ”¾ç½®è¯¥æ–‡ä»¶ã€‚"
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data, f"å·²åŠ è½½ {path}ï¼ˆ{len(data)} ä¸ªæ¡ç›®ï¼‰ã€‚"
    except Exception as e:
        return None, f"è¯»å– {path} å¤±è´¥ï¼š{_fmt_exception(e)}"

def eval_to_dataframe(data: Dict[str, Any]) -> pd.DataFrame:
    rows = []
    for name, v in data.items():
        rows.append({
            "model_name": name,
            "model_path": v.get("model_path"),
            "perplexity": v.get("perplexity"),
            "test_loss": v.get("test_loss"),
            "training_time_hours": v.get("training_time_hours"),
            "total_steps": v.get("total_steps"),
        })
    df = pd.DataFrame(rows)
    sort_cols = [c for c in ["perplexity", "test_loss"] if c in df.columns]
    if sort_cols:
        df = df.sort_values(by=sort_cols, ascending=[False]*len(sort_cols), na_position="last")
    return df

def pick_generation_samples(data: Dict[str, Any], model_name: str) -> List[Dict[str, str]]:
    item = data.get(model_name) or {}
    return item.get("generation_samples", []) or []

def run_evaluation(methods: str, runs_dir: str, base_model_hint: str, skip_perplexity: bool, max_samples: int) -> str:
    if not os.path.exists(EVAL_SCRIPT):
        return f"æœªæ‰¾åˆ°è¯„æµ‹è„šæœ¬ï¼š{EVAL_SCRIPT}"
    try:
        base_path = _resolve_local_model_path(base_model_hint)
    except Exception as e:
        return _fmt_exception(e)
    cmd = ["python", EVAL_SCRIPT, "--runs_dir", runs_dir, "--base_model", base_path, "--max_samples", str(int(max_samples))]
    parts = [p.strip() for p in (methods or "").split(",") if p.strip()]
    if parts:
        cmd += ["--methods"] + parts
    skip_perplexity = False
    if skip_perplexity:
        cmd += ["--skip_perplexity"]
    try:
        proc = subprocess.run(cmd, capture_output=True, text=True, check=False)
        out = []
        out.append("Command: " + " ".join(cmd))
        out.append("--- STDOUT ---\n" + (proc.stdout or ""))
        out.append("--- STDERR ---\n" + (proc.stderr or ""))
        return "\n".join(out)
    except Exception as e:
        return f"è¿è¡Œå¤±è´¥ï¼š{_fmt_exception(e)}"


def format_sft_text(messages: List[Dict[str, str]]) -> str:
    """å°†SFTæ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸ºæ–‡æœ¬"""
    text_parts = []
    for msg in messages:
        role = msg.get("role", "")
        content = msg.get("content", "")
        text_parts.append(f"{role}: {content}")
    return "\n".join(text_parts)

def save_to_parquet(data_text: str, output_path: str):
    """
    å°†ä¸åŒæ ¼å¼çš„æ–‡æœ¬æ•°æ®ä¿å­˜åˆ°parquetæ–‡ä»¶

    Args:
        data_text: åŒ…å«JSONæ ¼å¼æ•°æ®çš„æ–‡æœ¬å­—ç¬¦ä¸²
        output_path: è¾“å‡ºçš„parquetæ–‡ä»¶è·¯å¾„
    """
    # è§£ææ–‡æœ¬ä¸ºJSONå¯¹è±¡åˆ—è¡¨
    try:
        # å°è¯•è§£æä¸ºJSONæ•°ç»„
        data_list = json.loads(data_text)
        if not isinstance(data_list, list):
            data_list = [data_list]
    except json.JSONDecodeError:
        # å¦‚æœä¸æ˜¯æœ‰æ•ˆçš„JSONï¼Œå°è¯•é€è¡Œè§£æ
        lines = data_text.strip().split('\n')
        data_list = []
        for line in lines:
            line = line.strip()
            if line:
                try:
                    item = json.loads(line)
                    data_list.append(item)
                except json.JSONDecodeError:
                    print(f"è·³è¿‡æ— æ•ˆçš„JSONè¡Œ: {line}")
                    continue

    if not data_list:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„JSONæ•°æ®")

    # å¤„ç†æ¯ä¸ªæ•°æ®é¡¹
    processed_data = []

    for item in data_list:
        if not isinstance(item, dict):
            print(f"è·³è¿‡éå­—å…¸é¡¹: {item}")
            continue

        # è¯†åˆ«æ•°æ®ç±»å‹å¹¶æ ‡å‡†åŒ–æ ¼å¼
        if "messages" in item:
            # SFTæ ¼å¼: {"messages": [{"role":"user", "content":"..."}, {"role":"assistant", "content":"..."}]}
            processed_item = {
                "type": "sft",
                "messages": item["messages"],
                "text": format_sft_text(item["messages"])
            }

        elif "chosen" in item and "rejected" in item:
            # DPO/ORPOæ ¼å¼: {"prompt":"...", "chosen":"...", "rejected":"..."}
            processed_item = {
                "type": "dpo",
                "prompt": item.get("prompt", ""),
                "chosen": item.get("chosen", ""),
                "rejected": item.get("rejected", ""),
                "text": f"Prompt: {item.get('prompt', '')}\nChosen: {item.get('chosen', '')}\nRejected: {item.get('rejected', '')}"
            }

        elif "completion" in item and "label" in item:
            # KTOæ ¼å¼: {"prompt":"...", "completion":"...", "label": 1æˆ–0}
            processed_item = {
                "type": "kto",
                "prompt": item.get("prompt", ""),
                "completion": item.get("completion", ""),
                "label": item.get("label", 0),
                "text": f"Prompt: {item.get('prompt', '')}\nCompletion: {item.get('completion', '')}\nLabel: {item.get('label', 0)}"
            }
        else:
            print(f"æ— æ³•è¯†åˆ«çš„æ•°æ®æ ¼å¼: {item}")
            continue

        processed_data.append(processed_item)

    # åˆ›å»ºDataFrame
    df = pd.DataFrame(processed_data)

    # ä¿å­˜ä¸ºparquetæ–‡ä»¶
    df.to_parquet(output_path, index=False, engine='pyarrow')

# ------------------------- UI -------------------------
custom_css = """
.gradio-container {
    margin: 0 auto !important;
    max-width: 100% !important;
    width: 100% !important;
}
.container {
    max-width: 100% !important;
}
.main-title {
    text-align: center;
    margin-bottom: 20px;
}
.step-box {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    padding: 15px;
    border-radius: 10px;
    color: white;
    margin-bottom: 15px;
}
.info-box {
    background: #f0f7ff;
    padding: 12px;
    border-left: 4px solid #2196F3;
    border-radius: 5px;
    margin: 10px 0;
}
"""

with gr.Blocks(title="ğŸ¤– LLM å¾®è°ƒç³»ç»Ÿ", css=custom_css, theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    <div class="main-title">
    <h1>ğŸ¤– å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒç³»ç»Ÿ</h1>
    <p style="font-size: 16px; color: #666;">æœ¬åœ°ç¦»çº¿è¿è¡Œ | æ”¯æŒå¤šç§å¾®è°ƒæ–¹æ³• | å®Œæ•´è¯„æµ‹ä½“ç³»</p>
    </div>
    """)
    
    gr.Markdown("""
    <div class="info-box">
    ğŸ’¡ <b>ä½¿ç”¨è¯´æ˜ï¼š</b>æœ¬ç³»ç»Ÿå®Œå…¨ç¦»çº¿è¿è¡Œï¼ŒåŸºåº§æ¨¡å‹ä» <code>model/</code> ç›®å½•åŠ è½½ï¼Œå¾®è°ƒæƒé‡ä» <code>runs/</code> ç›®å½•é€‰æ‹©ã€‚
    </div>
    """)

    # ========== æ¨ç†å±•ç¤º ==========
    with gr.Tab("ğŸ’¬ å¯¹è¯ç”Ÿæˆ"):
        gr.Markdown("### ğŸ“ æ­¥éª¤ 1ï¼šé…ç½®æ¨¡å‹")

        with gr.Row():
            with gr.Column():
                base_model_hint = gr.Textbox(
                    value=DEFAULT_BASE_LOCAL,
                    label="ğŸ¯ åŸºåº§æ¨¡å‹è·¯å¾„",
                    placeholder="è¾“å…¥ model å­ç›®å½•åæˆ–å®Œæ•´è·¯å¾„",
                    info="ä¾‹å¦‚ï¼šQwen2-0.5B-Instruct æˆ–ç»å¯¹è·¯å¾„",
                    lines=1
                )
            with gr.Column():
                quant_mode = gr.Radio(
                    choices=["none", "8bit", "4bit"],
                    value="none",
                    label="ğŸ’¾ é‡åŒ–æ¨¡å¼",
                    info="é‡åŒ–å¯å‡å°‘æ˜¾å­˜å ç”¨"
                )

        gr.Markdown("### ğŸ“¦ æ­¥éª¤ 2ï¼šé€‰æ‹©å¾®è°ƒæƒé‡ï¼ˆå¯é€‰ï¼‰")

        runs_dir = gr.Textbox(
            value=RUNS_DIR,
            label="ğŸ“ æƒé‡ç›®å½•",
            lines=1
        )
        refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°æƒé‡åˆ—è¡¨", variant="secondary", size="sm")

        adapter_path = gr.State(value="")
        adapter_label = gr.Dropdown(
            choices=["(ä»…åŸºåº§æ¨¡å‹)"],
            value="(ä»…åŸºåº§æ¨¡å‹)",
            label="ğŸ¨ é€‰æ‹© LoRA é€‚é…å™¨",
            info="ç•™ç©ºåˆ™ä½¿ç”¨åŸºåº§æ¨¡å‹"
        )
        refresh_info = gr.Markdown("")

        gr.Markdown("### âš™ï¸ æ­¥éª¤ 3ï¼šè®¾ç½®ç”Ÿæˆå‚æ•°")

        with gr.Accordion("ğŸ”§ é«˜çº§å‚æ•°è®¾ç½®", open=False):
            with gr.Row():
                with gr.Column():
                    max_new_tokens = gr.Slider(
                        minimum=16, maximum=2048, step=16, value=256,
                        label="ğŸ“ æœ€å¤§ç”Ÿæˆé•¿åº¦",
                        info="ç”Ÿæˆçš„æœ€å¤§tokenæ•°"
                    )
                    temperature = gr.Slider(
                        minimum=0.0, maximum=2.0, step=0.1, value=0.7,
                        label="ğŸŒ¡ï¸ æ¸©åº¦ (Temperature)",
                        info="æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§ï¼Œè¶Šé«˜è¶Šéšæœº"
                    )
                with gr.Column():
                    top_p = gr.Slider(
                        minimum=0.1, maximum=1.0, step=0.05, value=0.9,
                        label="ğŸ² Top-p",
                        info="æ ¸é‡‡æ ·å‚æ•°ï¼Œæ§åˆ¶å¤šæ ·æ€§"
                    )
                    seed = gr.Number(
                        value=42, precision=0,
                        label="ğŸŒ± éšæœºç§å­",
                        info="å›ºå®šç§å­å¯å¤ç°ç»“æœ"
                    )

        gr.Markdown("### ğŸ’­ æ­¥éª¤ 4ï¼šè¾“å…¥é—®é¢˜å¹¶ç”Ÿæˆ")

        user_text = gr.Textbox(
            label="âœï¸ è¾“å…¥å†…å®¹ï¼ˆå•è½®å¯¹è¯ï¼‰",
            lines=5,
            placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œä¾‹å¦‚ï¼š\nâ€¢ ç»™æˆ‘3æ¡å­¦ä¹ ç¼–ç¨‹çš„å»ºè®®\nâ€¢ è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ \nâ€¢ å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
            info="æ”¯æŒå¤šè¡Œè¾“å…¥"
        )

        go = gr.Button("ğŸš€ å¼€å§‹ç”Ÿæˆ", variant="primary", size="lg")

        gr.Markdown("### ğŸ“¤ ç”Ÿæˆç»“æœ")

        with gr.Row():
            with gr.Column(scale=2):
                out_text = gr.Textbox(
                    label="ğŸ’¬ æ¨¡å‹å›å¤",
                    lines=12,
                    show_copy_button=True
                )
            with gr.Column(scale=1):
                out_lat = gr.Textbox(
                    label="â±ï¸ ç”Ÿæˆä¿¡æ¯",
                    lines=12
                )

        def _refresh(runs_dir_str: str):
            labels, paths = sync_runs_dropdown(runs_dir_str)
            # é»˜è®¤é€‰æ‹©ç¬¬ä¸€é¡¹ï¼ˆä»…åŸºåº§ï¼‰
            return gr.update(choices=labels, value=labels[0]), "" , f"å·²æ‰¾åˆ° {max(0, len(labels)-1)} ä¸ªé€‚é…å™¨ã€‚"

        refresh_btn.click(_refresh, inputs=[runs_dir], outputs=[adapter_label, adapter_path, refresh_info])

        def _choose_adapter(label: str, runs_dir_str: str):
            labels, paths = sync_runs_dropdown(runs_dir_str)
            mapping = {lbl: pth for lbl, pth in zip(labels, paths)}
            return mapping.get(label, "")

        adapter_label.change(_choose_adapter, inputs=[adapter_label, runs_dir], outputs=[adapter_path])

        go.click(
            infer_once,
            inputs=[base_model_hint, adapter_path, user_text, max_new_tokens, temperature, top_p, quant_mode, seed],
            outputs=[out_text, out_lat]
        )

    # ========== ç°åœºå¾®è°ƒ ==========
    with gr.Tab("ğŸ“ æ¨¡å‹è®­ç»ƒ"):
        gr.Markdown("""
        <div class="info-box">
        ğŸ¯ <b>å¿«é€Ÿå¾®è°ƒï¼š</b>ç²˜è´´å°‘é‡æ ·æœ¬æ•°æ®ï¼Œä¸€é”®å¯åŠ¨ LoRA/QLoRA å¾®è°ƒï¼Œç”Ÿæˆæ–°çš„æ¨¡å‹é€‚é…å™¨ã€‚
        </div>
        """)
        
        gr.Markdown("### ğŸ¯ æ­¥éª¤ 1ï¼šé€‰æ‹©å¾®è°ƒæ–¹æ³•")
        
        method2 = gr.Radio(
            choices=["sft","dpo","orpo","kto", 'grpo'],
            value="sft", 
            label="ğŸ”¬ å¾®è°ƒç®—æ³•",
            info="SFT=ç›‘ç£å¾®è°ƒ | DPO=ç›´æ¥åå¥½ä¼˜åŒ– | ORPO=å¥‡å¶æ¯”ä¼˜åŒ– | KTO=æ ‡ç­¾ä¼˜åŒ– | GRPO=ç¾¤ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–"
        )
        
        gr.Markdown("### ğŸ¯ æ­¥éª¤ 2ï¼šé…ç½®è®­ç»ƒå‚æ•°")
        
        with gr.Row():
            with gr.Column(scale=2):
                base_local = gr.Textbox(
                    value=DEFAULT_BASE_LOCAL, 
                    label="ğŸ¯ åŸºåº§æ¨¡å‹è·¯å¾„",
                    placeholder="ä¾‹å¦‚ï¼šQwen2-0.5B-Instruct",
                    info="ä» model/ ç›®å½•é€‰æ‹©æˆ–è¾“å…¥å®Œæ•´è·¯å¾„"
                )
            with gr.Column(scale=2):
                out_name = gr.Textbox(
                    value="runs/quick_sft", 
                    label="ğŸ’¾ è¾“å‡ºç›®å½•",
                    placeholder="ä¾‹å¦‚ï¼šruns/my_model",
                    info="è®­ç»ƒç»“æœä¿å­˜ä½ç½®ï¼ˆå»ºè®®åœ¨ runs/ ä¸‹ï¼‰"
                )

        with gr.Row():
            use_qlora = gr.Checkbox(
                value=True,
                label="âš¡ å¯ç”¨ QLoRA (4-bité‡åŒ–)",
                info="å¤§å¹…å‡å°‘æ˜¾å­˜å ç”¨ï¼Œæ¨èå¼€å¯"
            )
        
        gr.Markdown("### ğŸ“ æ­¥éª¤ 3ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®")
        gr.Markdown("""
        <div style="background: #fff3cd; padding: 10px; border-radius: 5px; border-left: 4px solid #ffc107;">
        ğŸ’¡ <b>æ•°æ®æ ¼å¼æç¤ºï¼š</b>
        <ul style="margin: 5px 0;">
        <li><b>SFTï¼š</b> å¯¹è¯æ ¼å¼ {"messages": [{"role":"user", "content":"..."}, {"role":"assistant", "content":"..."}]}</li>
        <li><b>DPO/ORPO/GRPOï¼š</b> åå¥½å¯¹ {"prompt":"...", "chosen":"...", "rejected":"..."}</li>
        <li><b>KTOï¼š</b> æ ‡æ³¨æ•°æ® {"prompt":"...", "completion":"...", "label": 1æˆ–0}</li>
        </ul>
        </div>
        """)
        
        data_json_txt = gr.Textbox(
            label="ğŸ“‹ è®­ç»ƒæ•°æ® (JSONæ ¼å¼)",
            lines=12,
            value='[{"messages":[{"role":"user","content":"ç»™æˆ‘3æ¡å­¦ä¹ ç¼–ç¨‹çš„å»ºè®®"},{"role":"assistant","content":"1. åšæŒç»ƒä¹ \\n2. é˜…è¯»æºç \\n3. å¤šåšé¡¹ç›®"}]}]',
            placeholder="ç²˜è´´ JSON æ ¼å¼çš„è®­ç»ƒæ•°æ®...",
            info="æ”¯æŒç²˜è´´æˆ–ç›´æ¥ç¼–è¾‘"
        )
        
        btn_fit = gr.Button("âš¡ å¼€å§‹è®­ç»ƒ", variant="primary", size="lg")
        
        gr.Markdown("### ğŸ“Š è®­ç»ƒæ—¥å¿—")
        
        fit_log = gr.Textbox(
            label="ğŸ“ å®æ—¶è®­ç»ƒæ—¥å¿—", 
            lines=20, 
            show_copy_button=True,
            placeholder="è®­ç»ƒæ—¥å¿—å°†åœ¨æ­¤å¤„æ˜¾ç¤º..."
        )
        
        btn_refresh_after = gr.Button(
            "ğŸ”„ è®­ç»ƒå®Œæˆååˆ·æ–°å¯¹è¯ç”Ÿæˆé¡µçš„æ¨¡å‹åˆ—è¡¨", 
            variant="secondary"
        )

        def _sample_fill(m):
            if m=="sft":
                return '[{"messages":[{"role":"user","content":"ç»™æˆ‘3æ¡å­¦ä¹ ç¼–ç¨‹çš„å»ºè®®"},{"role":"assistant","content":"1. åšæŒç»ƒä¹ \\n2. é˜…è¯»æºç \\n3. å¤šåšé¡¹ç›®"}]}]'
            if m in ("dpo","orpo"):
                return '[{"prompt":"å†™ä¸€æ®µè‡ªæˆ‘ä»‹ç»","chosen":"å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯â€¦ï¼ˆæ¸…æ™°æœ‰æ¡ç†ï¼‰","rejected":"å—¨â€¦ï¼ˆå«ç³Šå…¶è¾ï¼‰"}]'
            if m=="kto":
                return '[{"chosen":["ç»™æˆ‘ä¸€ä¸ªå­¦ä¹ è®¡åˆ’","å‘¨ä¸€åˆ°å‘¨äº”â€¦"],"rejected":["ç»™æˆ‘ä¸€ä¸ªå­¦ä¹ è®¡åˆ’","éšä¾¿å­¦ä¹ â€¦"]}]'
            if m=="grpo":
                return '''[{"prompt": "å†™ä¸€æ®µè‡ªæˆ‘ä»‹ç»","chosen": [{"role": "user", "content": "å†™ä¸€æ®µè‡ªæˆ‘ä»‹ç»"},{"role": "assistant", "content": "å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯â€¦ï¼ˆæ¸…æ™°æœ‰æ¡ç†ï¼‰"}],
                        "rejected": [{"role": "user", "content": "å†™ä¸€æ®µè‡ªæˆ‘ä»‹ç»"}, {"role": "assistant", "content": "å—¨â€¦ï¼ˆå«ç³Šå…¶è¾ï¼‰"}]}]'''
            return "[]"

        method2.change(_sample_fill, inputs=[method2], outputs=[data_json_txt])

        def _run_quick_fit(method, base_hint, out_dir, qlora_v, data_txt):
            print('Inputs: ', method, base_hint, out_dir, qlora_v, data_txt)

            import json, os, tempfile, subprocess
            try:
                base_path = _resolve_local_model_path(base_hint)
            except Exception as e:
                return f"è§£ææœ¬åœ°æ¨¡å‹å¤±è´¥ï¼š{e}"

            # --- è§£æå¹¶å†™å…¥ä¸´æ—¶ JSONï¼ˆç¡®ä¿ Windows ä¸é”æ–‡ä»¶ï¼‰ ---
            try:
                data = json.loads(data_txt)
                if not isinstance(data, list):
                    return "data_json å¿…é¡»æ˜¯ JSON åˆ—è¡¨ï¼ˆlist[dict]ï¼‰"
                fd, data_path = tempfile.mkstemp(suffix=".json")
                os.close(fd)  # å…³é”®ï¼šç«‹åˆ»å…³é—­å¥æŸ„ï¼Œé¿å… Windows æ–‡ä»¶é”
                with open(data_path, "w", encoding="utf-8") as f:
                    json.dump(data, f, ensure_ascii=False)
            except Exception as e:
                return f"è§£æ/å†™å…¥è®­ç»ƒæ•°æ®å¤±è´¥ï¼š{e}"

            if not os.path.exists(out_dir):
                try:
                    os.makedirs(out_dir, exist_ok=True)
                except Exception as e:
                    return f"åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥ï¼š{e}"
            data_path = f'{out_dir}/training_data.parquet'
            save_to_parquet(data_txt, data_path)

            # --- ç»„è£…å‘½ä»¤å¹¶è°ƒç”¨åç«¯ ---
            cmd = [
                "python", "code/SFT.py",
                "--method", method,
                "--model", base_path,
                "--dataset", data_path,   # ä¼ æœ¬åœ° JSON è·¯å¾„
                "--out", out_dir
            ]
            if qlora_v:
                cmd.append("--qlora")
            try:
                proc = subprocess.run(cmd, capture_output=True, text=True, check=False)
                out = []
                out.append("Command: " + " ".join(cmd))
                out.append("--- STDOUT ---\n" + (proc.stdout or ""))
                out.append("--- STDERR ---\n" + (proc.stderr or ""))
                return "\n".join(out)
            except Exception as e:
                return f"è¿è¡Œå¤±è´¥ï¼š{type(e).__name__}: {e}"
            finally:
                try:
                    os.remove(data_path)
                except Exception:
                    pass

        btn_fit.click(
            _run_quick_fit,
            inputs=[method2, base_local, out_name, use_qlora, data_json_txt],
            outputs=[fit_log]
        )

        # åˆ·æ–°æ¨ç†é¡µ Dropdown
        btn_refresh_after.click(_refresh, inputs=[runs_dir], outputs=[adapter_label, adapter_path, refresh_info])

    # ========== å¾®è°ƒå±•ç¤ºï¼ˆè¯„æµ‹ï¼‰ ==========
    with gr.Tab("ğŸ“ˆ æ¨¡å‹è¯„æµ‹"):
        gr.Markdown("""
        <div class="info-box">
        ğŸ“Š <b>å…¨é¢è¯„æµ‹ï¼š</b>è‡ªåŠ¨è¯„ä¼°æ‰€æœ‰è®­ç»ƒæ¨¡å‹ï¼Œæä¾›å›°æƒ‘åº¦ã€åå¥½å‡†ç¡®ç‡ç­‰å¤šç»´åº¦æŒ‡æ ‡ã€‚
        </div>
        """)
        
        gr.Markdown("### âš™ï¸ æ­¥éª¤ 1ï¼šé…ç½®è¯„æµ‹å‚æ•°")
        
        with gr.Row():
            with gr.Column(scale=2):
                methods = gr.Textbox(
                    value="", 
                    label="ğŸ” ç­›é€‰æ–¹æ³•ï¼ˆå¯é€‰ï¼‰",
                    placeholder="ä¾‹å¦‚ï¼šsft,dpo ï¼ˆç•™ç©ºè¯„æµ‹å…¨éƒ¨ï¼‰",
                    info="ç”¨é€—å·åˆ†éš”å¤šä¸ªæ–¹æ³•å"
                )
            with gr.Column(scale=2):
                runs_dir2 = gr.Textbox(
                    value=RUNS_DIR, 
                    label="ğŸ“ æ¨¡å‹ç›®å½•",
                    info="åŒ…å«è®­ç»ƒæ¨¡å‹çš„ç›®å½•"
                )
        
        with gr.Row():
            with gr.Column():
                base_model2 = gr.Textbox(
                    value=DEFAULT_BASE_LOCAL, 
                    label="ğŸ¯ åŸºåº§æ¨¡å‹è·¯å¾„",
                    info="ç”¨äºè¯„æµ‹çš„åŸºç¡€æ¨¡å‹"
                )
            with gr.Column():
                max_samples = gr.Slider(
                    minimum=20, maximum=1000, value=100, step=20,
                    label="ğŸ“Š è¯„æµ‹æ ·æœ¬æ•°",
                    info="æ¯ä¸ªæŒ‡æ ‡ä½¿ç”¨çš„æµ‹è¯•æ ·æœ¬æ•°é‡"
                )
            with gr.Column():
                skip_pp = gr.Checkbox(
                    value=True,
                    label="âš¡ å¿«é€Ÿæ¨¡å¼",
                    info="è·³è¿‡å›°æƒ‘åº¦è®¡ç®—ä»¥åŠ å¿«é€Ÿåº¦"
                )
        
        run_eval_btn = gr.Button("ğŸš€ å¼€å§‹è¯„æµ‹", variant="primary", size="lg")
        
        gr.Markdown("### ğŸ“‹ è¯„æµ‹æ—¥å¿—")
        
        cmd_log = gr.Textbox(
            label="ğŸ“ è¯„æµ‹æ‰§è¡Œæ—¥å¿—", 
            lines=15, 
            show_copy_button=True,
            placeholder="è¯„æµ‹æ—¥å¿—å°†åœ¨æ­¤å¤„æ˜¾ç¤º..."
        )
        
        reload_btn = gr.Button("ğŸ”„ é‡æ–°åŠ è½½è¯„æµ‹ç»“æœ", variant="secondary")
        status = gr.Markdown("")

        gr.Markdown("### ğŸ† æ¨¡å‹æ’è¡Œæ¦œ")
        
        leaderboard = gr.Dataframe(
            label="ğŸ“Š æ€§èƒ½å¯¹æ¯”è¡¨ï¼ˆæŒ‰åå¥½å‡†ç¡®ç‡å’ŒæŸå¤±å·®å€¼æ’åºï¼‰", 
            interactive=False,
            wrap=True
        )
        
        gr.Markdown("### ğŸ” ç”Ÿæˆæ ·ä¾‹æŸ¥çœ‹")
        
        with gr.Row():
            with gr.Column(scale=1):
                model_pick = gr.Dropdown(
                    choices=[], 
                    label="ğŸ¯ é€‰æ‹©æ¨¡å‹",
                    info="æŸ¥çœ‹è¯¥æ¨¡å‹çš„ç”Ÿæˆæ ·ä¾‹"
                )
            with gr.Column(scale=2):
                samples_box = gr.JSON(
                    label="ğŸ’¬ ç”Ÿæˆæ ·ä¾‹å±•ç¤º"
                )

        def _run_eval(methods_str, runs_dir_str, base_model_str, skip_pp_bool, max_samples_val):
            log = run_evaluation(methods_str, runs_dir_str, base_model_str, bool(skip_pp_bool), int(max_samples_val))
            return log

        run_eval_btn.click(_run_eval, inputs=[methods, runs_dir2, base_model2, skip_pp, max_samples], outputs=[cmd_log])

        def _reload_json():
            data, msg = try_read_eval_json(EVAL_JSON)
            if data is None:
                return gr.update(value=[]), gr.update(choices=[]), "[]", msg
            df = eval_to_dataframe(data)
            return df, gr.update(choices=list(data.keys()), value=(list(data.keys())[0] if data else None)), "[]", msg

        reload_btn.click(_reload_json, outputs=[leaderboard, model_pick, samples_box, status])

        def _pick_samples(model_name: str):
            data, _ = try_read_eval_json(EVAL_JSON)
            if not data or not model_name:
                return "[]"
            samples = pick_generation_samples(data, model_name)
            return json.dumps(samples, ensure_ascii=False, indent=2)

        model_pick.change(_pick_samples, inputs=[model_pick], outputs=[samples_box])

if __name__ == "__main__":
    import warnings
    warnings.filterwarnings("ignore", category=UserWarning)
    demo.launch(server_name="0.0.0.0", server_port=7860)
